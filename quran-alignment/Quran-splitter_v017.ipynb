{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74123262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1 ‚Äî FILENAME SCANNER SETUP (Imports + Helpers)\n",
    "\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from Surah_dict import SURAH_DICT\n",
    "\n",
    "with open(\"quran-no-tashkeel.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    QURAN_DATA = json.load(f)\n",
    "\n",
    "QURAN_FINGERPRINT = {}\n",
    "NORM = re.compile(r'[Ÿã-ŸëŸíŸ±ÿ£ÿ•ÿ¢Ÿâÿ©\\s]+')\n",
    "\n",
    "def clean(text):\n",
    "    return NORM.sub('', text)\n",
    "\n",
    "print(\"Building Quran fingerprint from your Surah_dict.py...\")\n",
    "\n",
    "for sid in SURAH_DICT.keys():\n",
    "    q = next(s for s in QURAN_DATA if f\"{s['id']:03d}\" == sid)\n",
    "    verses = q[\"verses\"]\n",
    "    start = 1 if q[\"id\"] not in [1, 9] else 0\n",
    "    words = []\n",
    "    for v in verses[start:start+8]:\n",
    "        words.extend(v[\"text\"].split())\n",
    "    fp = clean(' '.join(words[:18]))\n",
    "    \n",
    "    QURAN_FINGERPRINT[sid] = {\n",
    "        \"ar\": SURAH_DICT[sid][\"ar\"],\n",
    "        \"en\": SURAH_DICT[sid][\"en\"][0],\n",
    "        \"fp\": fp\n",
    "    }\n",
    "\n",
    "print(f\"Ready: {len(QURAN_FINGERPRINT)} surahs loaded (Bismillah skipped where needed)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76e9323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 2 ‚Äî Filename Pattern Matching & Surah Detection\n",
    "\n",
    "\"\"\"\n",
    "Setup for STRICT filename Surah detector.\n",
    "Includes imports, normalization, and lookup tables.\n",
    "Run this once per session.\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "from rapidfuzz import fuzz, process\n",
    "from Surah_dict import SURAH_DICT\n",
    "\n",
    "# ============================================================================\n",
    "# NORMALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "def normalize_arabic(text):\n",
    "    text = re.sub(r'[\\u064B-\\u0652\\u0670\\u0640]', '', text)\n",
    "    text = re.sub(r'[ÿ£ÿ•ÿ¢ÿß]', 'ÿß', text)\n",
    "    text = re.sub(r'ÿ©', 'Ÿá', text)\n",
    "    text = re.sub(r'Ÿâ]', 'Ÿä', text)\n",
    "    text = re.sub(r'[ÿ°]', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "def normalize_english(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[-_\\s]', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "# ============================================================================\n",
    "# BUILD LOOKUP TABLES\n",
    "# ============================================================================\n",
    "\n",
    "ARABIC_LOOKUP = {}\n",
    "ENGLISH_LOOKUP = {}\n",
    "NUMERIC_LOOKUP = {}\n",
    "\n",
    "for surah_id, data in SURAH_DICT.items():\n",
    "    # Arabic\n",
    "    ar_norm = normalize_arabic(data[\"ar\"])\n",
    "    ARABIC_LOOKUP[ar_norm] = surah_id\n",
    "\n",
    "    # English variants\n",
    "    for en in data[\"en\"]:\n",
    "        en_norm = normalize_english(en)\n",
    "        ENGLISH_LOOKUP[en_norm] = surah_id\n",
    "\n",
    "    # Numeric\n",
    "    NUMERIC_LOOKUP[str(int(surah_id))] = surah_id\n",
    "    NUMERIC_LOOKUP[surah_id] = surah_id\n",
    "\n",
    "ALL_ARABIC_NAMES = list(ARABIC_LOOKUP.keys())\n",
    "ALL_ENGLISH_NAMES = list(ENGLISH_LOOKUP.keys())\n",
    "\n",
    "print(f\"‚úì Loaded {len(SURAH_DICT)} surahs\")\n",
    "print(f\"  Arabic names: {len(ARABIC_LOOKUP)}\")\n",
    "print(f\"  English variants: {len(ENGLISH_LOOKUP)}\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# HELPER: Validate if text is a surah name (NOT a reciter)\n",
    "# ============================================================================\n",
    "\n",
    "def is_surah_name(name):\n",
    "    \"\"\"Check if name matches any surah name (Arabic or English)\"\"\"\n",
    "    if not name:\n",
    "        return False\n",
    "    name_norm = normalize_arabic(name) if any('\\u0600' <= c <= '\\u06FF' for c in name) else normalize_english(name)\n",
    "    return name_norm in ARABIC_LOOKUP or name_norm in ENGLISH_LOOKUP\n",
    "\n",
    "# ============================================================================\n",
    "# CASE DETECTORS\n",
    "# ============================================================================\n",
    "\n",
    "def extract_numeric_pattern(filename):\n",
    "    \"\"\"Extracts reliable numeric surah reference.\"\"\"\n",
    "    match = re.search(r'\\b(\\d{1,3})\\b', filename)\n",
    "    if not match:\n",
    "        return None\n",
    "    num = int(match.group(1))\n",
    "    if 1 <= num <= 114:\n",
    "        return NUMERIC_LOOKUP[str(num)]\n",
    "    return None\n",
    "\n",
    "def extract_reciter_pattern(filename):\n",
    "    \"\"\"Extract reciter + number, rejecting surah names as reciters\"\"\"\n",
    "    patterns = [\n",
    "        r'([A-Za-z\\u0600-\\u06FF]+)[-_](\\d{1,3})',  # reciter_sura (muzafar_003)\n",
    "        r'(\\d{1,3})[-_]([A-Za-z\\u0600-\\u06FF]+)',  # sura_reciter (003_muzafar)\n",
    "    ]\n",
    "    for pat in patterns:\n",
    "        m = re.search(pat, filename)\n",
    "        if m:\n",
    "            if len(m.groups()) == 2:\n",
    "                part1, part2 = m.groups()\n",
    "                if part1.isdigit():\n",
    "                    num, reciter = int(part1), part2\n",
    "                else:\n",
    "                    reciter, num = part1, int(part2)\n",
    "                if 1 <= num <= 114:\n",
    "                    # FIX: Don't return surah names as reciters\n",
    "                    if is_surah_name(reciter):\n",
    "                        return NUMERIC_LOOKUP[str(num)], None\n",
    "                    return NUMERIC_LOOKUP[str(num)], reciter.strip()\n",
    "    return None, None\n",
    "\n",
    "# ============================================================================\n",
    "# STRICT FUZZY MATCHERS\n",
    "# ============================================================================\n",
    "\n",
    "def fuzzy_match_arabic(text, threshold=85):\n",
    "    \"\"\"Arabic fuzzy disabled for any mixed tokens or digits.\"\"\"\n",
    "    if re.search(r'\\d', text):\n",
    "        return None, 0  # STRICT ‚Äî no fuzzy if digits exist\n",
    "\n",
    "    tokens = re.findall(r'[\\u0600-\\u06FF]+', text)\n",
    "    if not tokens:\n",
    "        return None, 0\n",
    "\n",
    "    best_id = None\n",
    "    best_score = 0\n",
    "\n",
    "    for t in tokens:\n",
    "        norm = normalize_arabic(t)\n",
    "        if len(norm) < 4:\n",
    "            continue  # too short to fuzzy accurately\n",
    "\n",
    "        match = process.extractOne(norm, ALL_ARABIC_NAMES, scorer=fuzz.ratio)\n",
    "        if match and match[1] >= threshold and match[1] > best_score:\n",
    "            best_id = ARABIC_LOOKUP[match[0]]\n",
    "            best_score = match[1]\n",
    "\n",
    "    return best_id, best_score\n",
    "\n",
    "def fuzzy_match_english(text, threshold=85):\n",
    "    if re.search(r'\\d', text):\n",
    "        return None, 0  # STRICT ‚Äî no fuzzy on mixed/digits\n",
    "\n",
    "    tokens = re.findall(r'[A-Za-z]+', text)\n",
    "    if not tokens:\n",
    "        return None, 0\n",
    "\n",
    "    best_id = None\n",
    "    best_score = 0\n",
    "\n",
    "    for t in tokens:\n",
    "        norm = normalize_english(t)\n",
    "        if len(norm) < 3:\n",
    "            continue\n",
    "\n",
    "        match = process.extractOne(norm, ALL_ENGLISH_NAMES, scorer=fuzz.ratio)\n",
    "        if match and match[1] >= threshold and match[1] > best_score:\n",
    "            best_id = ENGLISH_LOOKUP[match[0]]\n",
    "            best_score = match[1]\n",
    "\n",
    "    return best_id, best_score\n",
    "\n",
    "# ============================================================================\n",
    "# CONFLICT DETECTION\n",
    "# ============================================================================\n",
    "\n",
    "def detect_conflict(filename):\n",
    "    stem = Path(filename).stem\n",
    "\n",
    "    num_id = extract_numeric_pattern(stem)\n",
    "    if not num_id:\n",
    "        return False, None, None\n",
    "\n",
    "    name_ar, _ = fuzzy_match_arabic(stem, threshold=80)\n",
    "    name_en, _ = fuzzy_match_english(stem, threshold=80)\n",
    "    name_id = name_ar or name_en\n",
    "\n",
    "    if name_id and num_id != name_id:\n",
    "        return True, num_id, name_id\n",
    "\n",
    "    return False, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eba097b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 2-DEBUG ‚Äî Verify Pattern Matching Setup\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DEBUG: CELL 2 VERIFICATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Check lookups were built\n",
    "print(f\"\\n‚úì Lookup Tables:\")\n",
    "print(f\"   ARABIC_LOOKUP: {len(ARABIC_LOOKUP)} entries\")\n",
    "print(f\"   ENGLISH_LOOKUP: {len(ENGLISH_LOOKUP)} entries\")\n",
    "print(f\"   NUMERIC_LOOKUP: {len(NUMERIC_LOOKUP)} entries\")\n",
    "\n",
    "# 2. Check GLOBAL_RECITER was set\n",
    "if 'GLOBAL_RECITER' in globals():\n",
    "    print(f\"\\n‚úì GLOBAL_RECITER: '{GLOBAL_RECITER}'\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå ERROR: GLOBAL_RECITER not set!\")\n",
    "\n",
    "# 3. Test is_surah_name function\n",
    "print(f\"\\n‚úì Testing is_surah_name():\")\n",
    "test_cases = [\n",
    "    (\"ÿßŸÑŸÅÿßÿ™ÿ≠ÿ©\", True),\n",
    "    (\"Bakara\", True),\n",
    "    (\"Muzafar\", False),\n",
    "    (\"ŸÖÿ≠ŸÖÿØ\", True),  # This is actually both a name AND a surah!\n",
    "    (\"AlAfasy\", False)\n",
    "]\n",
    "for name, expected in test_cases:\n",
    "    result = is_surah_name(name)\n",
    "    status = \"‚úì\" if result == expected else \"‚ùå\"\n",
    "    print(f\"   {status} is_surah_name('{name}') = {result} (expected {expected})\")\n",
    "\n",
    "# 4. Test extract_reciter_pattern\n",
    "print(f\"\\n‚úì Testing extract_reciter_pattern():\")\n",
    "test_files = [\n",
    "    \"Muzafar_003.mp3\",\n",
    "    \"ÿßŸÑŸÅÿßÿ™ÿ≠ÿ©_001.mp3\",  # Should reject ÿßŸÑŸÅÿßÿ™ÿ≠ÿ© as reciter\n",
    "    \"003_AlAfasy.mp3\",\n",
    "    \"Bakara_002.mp3\"    # Should reject Bakara as reciter\n",
    "]\n",
    "for fname in test_files:\n",
    "    sid, reciter = extract_reciter_pattern(fname)\n",
    "    print(f\"   {fname}\")\n",
    "    print(f\"      Surah: {sid}, Reciter: {reciter}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247fe1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 3 ‚Äî FILENAME SCANNER EXECUTION (Matcher + Scan)\n",
    "\n",
    "\"\"\"\n",
    "Main execution for STRICT filename Surah detector.\n",
    "Uses helpers from Cell 1.5A.\n",
    "Re-run this for new files.\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN MATCHER\n",
    "# ============================================================================\n",
    "\n",
    "def match_filename_to_surah(filename, threshold=85):\n",
    "    stem = Path(filename).stem\n",
    "\n",
    "    # Priority 1 ‚Äî deterministic numeric\n",
    "    num_id = extract_numeric_pattern(stem)\n",
    "    if num_id:\n",
    "        return num_id, 100, \"numeric\", None  # No reciter\n",
    "\n",
    "    # Priority 2 ‚Äî reciter style numeric, extract reciter\n",
    "    rec_id, reciter_name = extract_reciter_pattern(stem)\n",
    "    if rec_id:\n",
    "        return rec_id, 95, \"reciter_pattern\", reciter_name\n",
    "\n",
    "    # Priority 3 ‚Äî Arabic fuzzy\n",
    "    ar_id, score = fuzzy_match_arabic(stem, threshold)\n",
    "    if ar_id:\n",
    "        return ar_id, score, \"arabic_fuzzy\", None\n",
    "\n",
    "    # Priority 4 ‚Äî English fuzzy\n",
    "    en_id, score = fuzzy_match_english(stem, threshold)\n",
    "    if en_id:\n",
    "        return en_id, score, \"english_fuzzy\", None\n",
    "\n",
    "    return None, 0, None, None\n",
    "\n",
    "# ============================================================================\n",
    "# BATCH SCAN\n",
    "# ============================================================================\n",
    "\n",
    "def scan_audio_files(directory=\".\", threshold=85):\n",
    "    audio_exts = {\".mp3\", \".wav\", \".m4a\", \".flac\", \".ogg\"}\n",
    "    files = [f for f in Path(directory).iterdir()\n",
    "             if f.suffix.lower() in audio_exts]\n",
    "\n",
    "    matched = []\n",
    "    unmatched = []\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(\"STRICT SCAN: AUDIO FILES\")\n",
    "    print(\"=\"*80, \"\\n\")\n",
    "\n",
    "    for f in sorted(files):\n",
    "        is_conflict, num_says, name_says = detect_conflict(f.name)\n",
    "        if is_conflict:\n",
    "            print(f\"‚ö†Ô∏è  {f.name} ‚Äî CONFLICT: number={num_says} vs name={name_says}\")\n",
    "            unmatched.append(f)\n",
    "            continue\n",
    "\n",
    "        surah_id, conf, method, reciter = match_filename_to_surah(f.name, threshold)\n",
    "\n",
    "        if surah_id:\n",
    "            data = SURAH_DICT[surah_id]\n",
    "            matched.append({\n",
    "                \"file\": f,\n",
    "                \"surah_id\": surah_id,\n",
    "                \"surah_name_ar\": data[\"ar\"],\n",
    "                \"surah_name_en\": data[\"en\"][0],\n",
    "                \"confidence\": conf,\n",
    "                \"method\": method,\n",
    "                \"reciter\": reciter  # None if not extracted\n",
    "            })\n",
    "\n",
    "            emoji = \"üéØ\" if conf == 100 else \"‚úì\"\n",
    "            print(f\"{emoji} {f.name}\")\n",
    "            print(f\"   ‚Üí {surah_id} {data['ar']} ({data['en'][0]})\")\n",
    "            print(f\"   Confidence: {conf}% | Method: {method}\")\n",
    "            if reciter:\n",
    "                print(f\"   Extracted reciter: {reciter}\")\n",
    "            print()\n",
    "\n",
    "        else:\n",
    "            print(f\"‚ùå {f.name}\")\n",
    "            print(\"   ‚Üí No match found (Whisper required)\\n\")\n",
    "            unmatched.append(f)\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(f\"MATCHED: {len(matched)} | REQUIRE WHISPER: {len(unmatched)}\")\n",
    "    print(\"=\"*80, \"\\n\")\n",
    "\n",
    "    return matched, unmatched\n",
    "\n",
    "# ============================================================================\n",
    "# EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    matched, unmatched = scan_audio_files(threshold=85)\n",
    "\n",
    "    # ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "    # SMART SPLIT: SKIP WHISPER WHEN 100% SURE\n",
    "    # ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "    HIGH_CONF_FILES = []      # These skip Whisper ‚Üí go straight to folder\n",
    "    NEED_WHISPER_FILES = []   # Truly unknown names only\n",
    "\n",
    "    for item in matched:\n",
    "        if (item[\"confidence\"] >= 97 or \n",
    "            item[\"method\"] in [\"numeric\", \"reciter_pattern\"]):\n",
    "            HIGH_CONF_FILES.append({\n",
    "                \"path\": str(item[\"file\"]),\n",
    "                \"surah_id\": item[\"surah_id\"],\n",
    "                \"ar\": item[\"surah_name_ar\"],\n",
    "                \"en\": item[\"surah_name_en\"],\n",
    "                \"audio_file\": item[\"file\"].name,\n",
    "                \"base_name\": f\"{item['surah_name_en'].replace(' ', '_')}_{item['surah_id'].lstrip('0')}\",\n",
    "                \"reciter\": item[\"reciter\"]\n",
    "            })\n",
    "        else:\n",
    "            NEED_WHISPER_FILES.append(item[\"file\"])\n",
    "\n",
    "    WHISPER_REQUIRED_FILES = unmatched + NEED_WHISPER_FILES\n",
    "\n",
    "    print(f\"\\nSKIP WHISPER ‚Üí {len(HIGH_CONF_FILES)} files (bakara.mp3, 002.mp3, muzafar_003.mp3, etc.)\")\n",
    "    print(f\"NEED WHISPER ‚Üí {len(WHISPER_REQUIRED_FILES)} files\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b80d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 3-DEBUG ‚Äî Verify File Scanning Results\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DEBUG: CELL 3 VERIFICATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Check HIGH_CONF_FILES\n",
    "if 'HIGH_CONF_FILES' in globals():\n",
    "    print(f\"\\n‚úì HIGH_CONF_FILES: {len(HIGH_CONF_FILES)} files\")\n",
    "    \n",
    "    # Show first 3\n",
    "    for i, item in enumerate(HIGH_CONF_FILES[:3], 1):\n",
    "        print(f\"\\n   {i}. {item['audio_file']}\")\n",
    "        print(f\"      Surah: {item['surah_id']} - {item['ar']}\")\n",
    "        print(f\"      Extracted reciter: {item.get('reciter', 'None')}\")\n",
    "        print(f\"      Path: {item['path']}\")\n",
    "    \n",
    "    if len(HIGH_CONF_FILES) > 3:\n",
    "        print(f\"\\n   ... and {len(HIGH_CONF_FILES) - 3} more files\")\n",
    "    \n",
    "    # Check for surah names in reciter field\n",
    "    print(f\"\\n‚úì Checking for surah names incorrectly marked as reciters:\")\n",
    "    surah_as_reciter = [item for item in HIGH_CONF_FILES \n",
    "                        if item.get('reciter') and is_surah_name(item['reciter'])]\n",
    "    if surah_as_reciter:\n",
    "        print(f\"   ‚ùå FOUND {len(surah_as_reciter)} files with surah names as reciters!\")\n",
    "        for item in surah_as_reciter[:3]:\n",
    "            print(f\"      ‚Ä¢ {item['audio_file']} ‚Üí reciter='{item['reciter']}'\")\n",
    "    else:\n",
    "        print(f\"   ‚úì No surah names incorrectly marked as reciters\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå ERROR: HIGH_CONF_FILES not created!\")\n",
    "\n",
    "# 2. Check WHISPER_REQUIRED_FILES\n",
    "if 'WHISPER_REQUIRED_FILES' in globals():\n",
    "    print(f\"\\n‚úì WHISPER_REQUIRED_FILES: {len(WHISPER_REQUIRED_FILES)} files\")\n",
    "    if WHISPER_REQUIRED_FILES:\n",
    "        print(f\"   Files needing Whisper:\")\n",
    "        for f in WHISPER_REQUIRED_FILES[:5]:\n",
    "            print(f\"      ‚Ä¢ {f.name}\")\n",
    "        if len(WHISPER_REQUIRED_FILES) > 5:\n",
    "            print(f\"      ... and {len(WHISPER_REQUIRED_FILES) - 5} more\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå ERROR: WHISPER_REQUIRED_FILES not created!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2315e773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 4 ‚Äî FOLDER ORGANIZATION ONLY (NO WHISPER)\n",
    "\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "processed_surahs = []\n",
    "skipped_files = []\n",
    "\n",
    "# ==============================================================\n",
    "# HELPER: Check if extracted reciter is actually a surah name\n",
    "# ==============================================================\n",
    "def is_surah_name(name):\n",
    "    \"\"\"Check if name matches any surah name (Arabic or English)\"\"\"\n",
    "    if not name:\n",
    "        return False\n",
    "    name_norm = normalize_arabic(name) if any('\\u0600' <= c <= '\\u06FF' for c in name) else normalize_english(name)\n",
    "    \n",
    "    for surah_id, data in SURAH_DICT.items():\n",
    "        if normalize_arabic(data[\"ar\"]) == name_norm:\n",
    "            return True\n",
    "        for en in data[\"en\"]:\n",
    "            if normalize_english(en) == name_norm:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# ==============================================================\n",
    "# CHECK IF FILES EXIST\n",
    "# ==============================================================\n",
    "if 'HIGH_CONF_FILES' not in globals() or not HIGH_CONF_FILES:\n",
    "    print(\"‚ùå No files matched! Run Cell 1.5B first.\")\n",
    "    raise RuntimeError(\"No HIGH_CONF_FILES found\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"ORGANIZING {len(HIGH_CONF_FILES)} MATCHED FILES...\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# ==============================================================\n",
    "# STEP 1: AUTO-DETECT RECITER (OR ASK ONCE)\n",
    "# ==============================================================\n",
    "auto_reciter = None\n",
    "\n",
    "# Look for valid reciter name in ANY file\n",
    "for item in HIGH_CONF_FILES:\n",
    "    if item[\"reciter\"] and not is_surah_name(item[\"reciter\"]):\n",
    "        auto_reciter = item[\"reciter\"].capitalize()\n",
    "        print(f\"‚úì Auto-detected reciter: {auto_reciter}\\n\")\n",
    "        break\n",
    "\n",
    "# If no reciter found, ask ONCE for ALL files\n",
    "if not auto_reciter:\n",
    "    auto_reciter = input(\"Reciter name for ALL files (press Enter for 'Unknown'): \").strip() or \"Unknown\"\n",
    "    auto_reciter = auto_reciter.capitalize()\n",
    "    print()\n",
    "\n",
    "# Create reciter folder\n",
    "reciter_folder = Path(auto_reciter)\n",
    "reciter_folder.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"‚Üí Saving to: {auto_reciter}/\")\n",
    "print(f\"‚Üí Format: SurahName_000_{auto_reciter}.mp3\\n\")\n",
    "\n",
    "# ==============================================================\n",
    "# STEP 2: ORGANIZE ALL FILES (ONE LOOP, NO DUPLICATES)\n",
    "# ==============================================================\n",
    "processed_paths = set()\n",
    "\n",
    "for item in HIGH_CONF_FILES:\n",
    "    # Skip if already processed\n",
    "    if item[\"path\"] in processed_paths:\n",
    "        print(f\"‚ö† SKIP (duplicate): {item['path']}\")\n",
    "        continue\n",
    "    \n",
    "    sid = item[\"surah_id\"]\n",
    "    \n",
    "    # FLAT structure: Muzafar/imran_003_Muzafar.mp3\n",
    "    new_name = f\"{item['en'].replace(' ', '_')}_{sid}_{auto_reciter}{Path(item['path']).suffix}\"\n",
    "    new_path = reciter_folder / new_name\n",
    "    \n",
    "    # Skip if file already exists\n",
    "    if new_path.exists():\n",
    "        skipped_files.append(str(new_path))\n",
    "        print(f\"‚ö† SKIP (exists): {new_path}\")\n",
    "        continue\n",
    "    \n",
    "    # Copy file\n",
    "    shutil.copy2(item[\"path\"], new_path)\n",
    "    processed_paths.add(item[\"path\"])\n",
    "    \n",
    "    # Add to processed list for Cell 2C\n",
    "    processed_surahs.append({\n",
    "        \"sid\": sid,\n",
    "        \"ar\": item[\"ar\"],\n",
    "        \"en\": item[\"en\"],\n",
    "        \"organized_audio_path\": str(new_path),\n",
    "        \"audio_file\": new_name,\n",
    "        \"base_name\": f\"{item['en'].replace(' ', '_')}_{sid}_{auto_reciter}\",\n",
    "        \"path\": item[\"path\"],\n",
    "        \"source\": \"filename_high_confidence\"\n",
    "    })\n",
    "    \n",
    "    print(f\"‚úì {sid} {item['ar']} ‚Üí {new_name}\")\n",
    "\n",
    "# ==============================================================\n",
    "# SUMMARY\n",
    "# ==============================================================\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"‚úì {len(processed_surahs)} files organized in: {reciter_folder}/\")\n",
    "if skipped_files:\n",
    "    print(f\"‚ö† Skipped {len(skipped_files)} existing files\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\n‚ú® Ready for Cell 2C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1ecd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 5 ‚Äî FIXED: OpenAI Whisper + Duration Correct\n",
    "import torch\n",
    "import gc\n",
    "import whisper\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print(f\"RTX 5070 clean start\")\n",
    "\n",
    "if 'processed_surahs' not in globals() or not processed_surahs:\n",
    "    raise RuntimeError(\"Run CELL 4 first!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRANSCRIPTION ‚Äî OPENAI WHISPER (RTX 5070 GUARANTEED)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "model = whisper.load_model(\"medium\", device=\"cuda\")  # Medium = fast, stable\n",
    "print(\"Model loaded\")\n",
    "\n",
    "for item in processed_surahs:\n",
    "    audio_path = Path(item['organized_audio_path'])\n",
    "    if not audio_path.exists():\n",
    "        print(f\"Missing: {audio_path}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nSurah: {item['sid']} {item['ar']} | {audio_path.name}\")\n",
    "    \n",
    "    result = model.transcribe(\n",
    "        str(audio_path),\n",
    "        language=\"ar\",\n",
    "        word_timestamps=True,\n",
    "        no_speech_threshold=0.2,\n",
    "        logprob_threshold=-1.0,  # Even more permissive\n",
    "        compression_ratio_threshold=2.0,\n",
    "        condition_on_previous_text=False  # Don't stop early\n",
    "    )\n",
    "    \n",
    "    words = []\n",
    "    for seg in tqdm(result[\"segments\"], desc=\"Words\"):\n",
    "        for w in seg.get(\"words\", []):\n",
    "            words.append({\n",
    "                \"word\": w[\"word\"].strip(),\n",
    "                \"start_ms\": int(w[\"start\"] * 1000),\n",
    "                \"end_ms\": int(w[\"end\"] * 1000),\n",
    "                \"confidence\": round(w[\"probability\"], 4)\n",
    "            })\n",
    "    \n",
    "    json_file = audio_path.parent / f\"{item['base_name']}_DELETE.json\"\n",
    "\n",
    "    metadata = {\n",
    "        \"surah_id\": item['sid'],\n",
    "        \"surah_name_ar\": item['ar'],\n",
    "        \"surah_name_en\": item['en'],\n",
    "        \"audio_file\": audio_path.name,\n",
    "        \"base_name\": item['base_name'],\n",
    "        \"duration_seconds\": round(result['segments'][-1]['end'], 2),  # ‚Üê ACTUALLY FIXED NOW!\n",
    "        \"total_words\": len(words),\n",
    "        \"model\": \"openai-whisper medium\"\n",
    "    }\n",
    "    \n",
    "    with open(json_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"metadata\": metadata, \"words\": words}, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"   Words: {len(words):,} | Duration: {metadata['duration_seconds']/60:.1f} min | Saved: {json_file.name}\")\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e552ce3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 5.5 ‚Äî Extract Clean Quran Text (CORRECT VERSION)\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "if 'processed_surahs' not in globals() or not processed_surahs:\n",
    "    raise RuntimeError(\"‚ùå Run Cell 4 first!\")\n",
    "\n",
    "# Find reciter folder\n",
    "if 'GLOBAL_RECITER' in globals():\n",
    "    reciter_folder = Path(GLOBAL_RECITER)\n",
    "else:\n",
    "    first_path = Path(processed_surahs[0]['organized_audio_path'])\n",
    "    reciter_folder = first_path.parent\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXTRACTING CLEAN QURAN TEXT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load Quran data\n",
    "with open(\"quran-no-tashkeel.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    QURAN = json.load(f)\n",
    "\n",
    "for item in processed_surahs:\n",
    "    sid = item['sid']\n",
    "    base_name = item['base_name']\n",
    "    \n",
    "    surah_num = int(sid)\n",
    "    surah_data = next((s for s in QURAN if s['id'] == surah_num), None)\n",
    "    \n",
    "    if not surah_data:\n",
    "        print(f\"‚ùå Surah {sid} not found!\")\n",
    "        continue\n",
    "    \n",
    "    # Include ALL verses\n",
    "    clean_txt = reciter_folder / f\"{base_name}_CLEAN.txt\"\n",
    "    \n",
    "    with open(clean_txt, 'w', encoding='utf-8') as f:\n",
    "        for verse in surah_data['verses']:\n",
    "            f.write(f\"({verse['id']}) {verse['text']}\\n\")\n",
    "    \n",
    "    print(f\"‚úì {sid} {item['ar']} ‚Üí {len(surah_data['verses'])} ayas\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"‚úÖ Clean text files created!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc1c8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 6 ‚Äî ROBUST AYA ALIGNMENT WITH CONFIDENCE SCORING\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "from rapidfuzz import fuzz\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "print(\"AUTO ALIGNMENT: aya anchors + smart gap filling + confidence scoring\\n\")\n",
    "\n",
    "aligned = 0\n",
    "processed = 0\n",
    "\n",
    "# ============================================================================\n",
    "# NORMALIZATION\n",
    "# ============================================================================\n",
    "def normalize(text: str) -> str:\n",
    "    \"\"\"Normalize Arabic text for matching\"\"\"\n",
    "    text = re.sub(r'[\\u064B-\\u0652\\u0670]', '', text)  # Remove diacritics\n",
    "    text = re.sub(r'[ÿ£ÿ•ÿ¢]', 'ÿß', text)  # Normalize alif\n",
    "    text = re.sub(r'ÿ©', 'Ÿá', text)  # Normalize ta marbuta\n",
    "    text = re.sub(r'Ÿâ', 'Ÿä', text)  # Normalize alif maksura\n",
    "    return text.strip()\n",
    "\n",
    "# ============================================================================\n",
    "# ANCHOR MATCHING WITH CONFIDENCE - MULTI-WORD CONTEXT\n",
    "# ============================================================================\n",
    "def find_aya_anchor(\n",
    "    aya_words: List[str], \n",
    "    whisper_words: List[Dict], \n",
    "    start_search_pos: int,\n",
    "    search_window: int = 20\n",
    ") -> Tuple[int, float]:\n",
    "    \"\"\"\n",
    "    Find the best starting position for an aya in the whisper transcript.\n",
    "    Uses multi-word context (first 3 words) for more accurate matching.\n",
    "    Returns (position, confidence_score)\n",
    "    \"\"\"\n",
    "    if not aya_words or start_search_pos >= len(whisper_words):\n",
    "        return start_search_pos, 0.0\n",
    "    \n",
    "    # Use first 3 words for context (more reliable than single word)\n",
    "    context_size = min(3, len(aya_words))\n",
    "    aya_context = \" \".join(normalize(w) for w in aya_words[:context_size])\n",
    "    \n",
    "    best_pos = start_search_pos\n",
    "    best_score = 0.0\n",
    "    \n",
    "    # Search within window for best match\n",
    "    search_end = min(start_search_pos + search_window, len(whisper_words))\n",
    "    \n",
    "    for i in range(start_search_pos, search_end):\n",
    "        # Build whisper context of same size\n",
    "        whisper_context_words = []\n",
    "        for j in range(i, min(i + context_size, len(whisper_words))):\n",
    "            whisper_context_words.append(normalize(whisper_words[j][\"word\"]))\n",
    "        \n",
    "        whisper_context = \" \".join(whisper_context_words)\n",
    "        \n",
    "        # Compare contexts\n",
    "        score = fuzz.ratio(whisper_context, aya_context)\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_pos = i\n",
    "            \n",
    "            # Early exit for excellent match\n",
    "            if score >= 85:  # Lowered threshold for multi-word\n",
    "                break\n",
    "    \n",
    "    # Convert to 0-1 confidence scale\n",
    "    confidence = best_score / 100.0\n",
    "    \n",
    "    return best_pos, confidence\n",
    "\n",
    "# ============================================================================\n",
    "# WORD ALIGNMENT WITH CONFIDENCE\n",
    "# ============================================================================\n",
    "def align_words_with_confidence(\n",
    "    quran_words: List[str],\n",
    "    whisper_words: List[Dict],\n",
    "    whisper_start_pos: int\n",
    ") -> Tuple[List[Dict], float]:\n",
    "    \"\"\"\n",
    "    Align Quran words to Whisper words with confidence scoring.\n",
    "    Returns (aligned_words, average_confidence)\n",
    "    \"\"\"\n",
    "    aligned = []\n",
    "    confidences = []\n",
    "    whisper_pos = whisper_start_pos\n",
    "    \n",
    "    for quran_word in quran_words:\n",
    "        if whisper_pos >= len(whisper_words):\n",
    "            # Out of whisper words - will need gap filling\n",
    "            break\n",
    "        \n",
    "        whisper_word = whisper_words[whisper_pos]\n",
    "        quran_norm = normalize(quran_word)\n",
    "        whisper_norm = normalize(whisper_word[\"word\"])\n",
    "        \n",
    "        # Calculate match confidence\n",
    "        match_score = fuzz.ratio(quran_norm, whisper_norm) / 100.0\n",
    "        \n",
    "        # Combine with whisper's own confidence\n",
    "        whisper_confidence = whisper_word.get(\"confidence\", 0.95)\n",
    "        combined_confidence = (match_score * 0.7) + (whisper_confidence * 0.3)\n",
    "        \n",
    "        aligned.append({\n",
    "            \"word\": quran_word,\n",
    "            \"start_ms\": whisper_word[\"start_ms\"],\n",
    "            \"end_ms\": whisper_word[\"end_ms\"],\n",
    "            \"confidence\": round(combined_confidence, 4),\n",
    "            \"matched\": match_score > 0.6  # Flag if direct match\n",
    "        })\n",
    "        \n",
    "        confidences.append(combined_confidence)\n",
    "        whisper_pos += 1\n",
    "    \n",
    "    avg_confidence = sum(confidences) / len(confidences) if confidences else 0.0\n",
    "    \n",
    "    return aligned, avg_confidence, whisper_pos\n",
    "\n",
    "# ============================================================================\n",
    "# SMART GAP FILLING\n",
    "# ============================================================================\n",
    "def fill_missing_words(\n",
    "    aya_words: List[str],\n",
    "    aligned_words: List[Dict],\n",
    "    whisper_words: List[Dict],\n",
    "    next_whisper_pos: int\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Fill in missing words at the end of an aya with intelligent timing.\n",
    "    Returns complete list of aligned words.\n",
    "    \"\"\"\n",
    "    remaining_words = aya_words[len(aligned_words):]\n",
    "    \n",
    "    if not remaining_words:\n",
    "        return aligned_words\n",
    "    \n",
    "    # Case 1: We have some aligned words - extend from last word\n",
    "    if aligned_words:\n",
    "        last_word = aligned_words[-1]\n",
    "        last_end = last_word[\"end_ms\"]\n",
    "        \n",
    "        # Calculate average word duration from aligned words\n",
    "        durations = [w[\"end_ms\"] - w[\"start_ms\"] for w in aligned_words]\n",
    "        avg_duration = sum(durations) // len(durations) if durations else 500\n",
    "        \n",
    "        # Find next boundary\n",
    "        if next_whisper_pos < len(whisper_words):\n",
    "            next_start = whisper_words[next_whisper_pos][\"start_ms\"]\n",
    "            available_time = max(next_start - last_end, 0)\n",
    "        else:\n",
    "            # Use average duration for remaining words\n",
    "            available_time = avg_duration * len(remaining_words)\n",
    "        \n",
    "        # Distribute time evenly\n",
    "        word_duration = available_time // len(remaining_words) if available_time > 0 else avg_duration\n",
    "        word_duration = max(word_duration, 300)  # Minimum 300ms per word\n",
    "        \n",
    "        for i, word in enumerate(remaining_words):\n",
    "            word_start = last_end + (word_duration * i)\n",
    "            word_end = word_start + word_duration\n",
    "            \n",
    "            aligned_words.append({\n",
    "                \"word\": word,\n",
    "                \"start_ms\": word_start,\n",
    "                \"end_ms\": word_end,\n",
    "                \"confidence\": 0.30,  # Low confidence for gap-filled words\n",
    "                \"matched\": False\n",
    "            })\n",
    "    \n",
    "    # Case 2: No aligned words at all - complete aya missing\n",
    "    else:\n",
    "        # Estimate based on typical word duration\n",
    "        default_duration = 600  # 600ms per word\n",
    "        \n",
    "        # Try to anchor to next available whisper word\n",
    "        if next_whisper_pos < len(whisper_words):\n",
    "            anchor_time = whisper_words[next_whisper_pos][\"start_ms\"]\n",
    "            # Work backwards\n",
    "            total_duration = default_duration * len(remaining_words)\n",
    "            start_time = max(0, anchor_time - total_duration)\n",
    "        else:\n",
    "            # No anchor - use 0 or last known time\n",
    "            start_time = 0\n",
    "        \n",
    "        for i, word in enumerate(remaining_words):\n",
    "            word_start = start_time + (default_duration * i)\n",
    "            word_end = word_start + default_duration\n",
    "            \n",
    "            aligned_words.append({\n",
    "                \"word\": word,\n",
    "                \"start_ms\": word_start,\n",
    "                \"end_ms\": word_end,\n",
    "                \"confidence\": 0.20,  # Very low confidence\n",
    "                \"matched\": False\n",
    "            })\n",
    "    \n",
    "    return aligned_words\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN ALIGNMENT LOOP\n",
    "# ============================================================================\n",
    "for whisper_json in sorted(Path(\".\").rglob(\"*_DELETE.json\")):\n",
    "    aligned_json = whisper_json.with_name(\n",
    "        whisper_json.stem.replace(\"_DELETE\", \"\") + \"_aligned.json\"\n",
    "    )\n",
    "    \n",
    "    if aligned_json.exists():\n",
    "        aligned += 1\n",
    "        continue\n",
    "    \n",
    "    processed += 1\n",
    "    print(f\"Aligning ‚Üí {whisper_json.name}\")\n",
    "\n",
    "    # Load whisper transcription\n",
    "    try:\n",
    "        with open(whisper_json, encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            whisper_words = data.get(\"words\", [])\n",
    "            metadata = data.get(\"metadata\", {})\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå ERROR loading {whisper_json.name}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Load clean Quran text\n",
    "    clean_txt = whisper_json.with_name(\n",
    "        whisper_json.stem.replace(\"_DELETE\", \"\") + \"_CLEAN.txt\"\n",
    "    )\n",
    "    \n",
    "    if not clean_txt.exists():\n",
    "        print(f\"  ‚ùå ERROR: {clean_txt.name} not found!\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        with open(clean_txt, encoding=\"utf-8\") as f:\n",
    "            clean_lines = [\n",
    "                re.sub(r\"^\\(\\d+\\)\\s*\", \"\", l.strip()) \n",
    "                for l in f if l.strip()\n",
    "            ]\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå ERROR loading {clean_txt.name}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Validate data\n",
    "    if not whisper_words:\n",
    "        print(f\"  ‚ö†Ô∏è  WARNING: No whisper words found!\")\n",
    "        continue\n",
    "    \n",
    "    if not clean_lines:\n",
    "        print(f\"  ‚ö†Ô∏è  WARNING: No Quran text found!\")\n",
    "        continue\n",
    "\n",
    "    # ========================================================================\n",
    "    # ALIGN EACH AYA\n",
    "    # ========================================================================\n",
    "    ayas = []\n",
    "    global_whisper_pos = 0\n",
    "    low_confidence_ayas = []\n",
    "    \n",
    "    for aya_num, aya_text in enumerate(clean_lines, 1):\n",
    "        aya_words = aya_text.split()\n",
    "        \n",
    "        if not aya_words:\n",
    "            continue\n",
    "        \n",
    "        # STEP 0: Validate with previous aya's end boundary\n",
    "        expected_min_start = 0\n",
    "        if ayas:  # We have previous aya\n",
    "            prev_end = ayas[-1][\"end_ms\"]\n",
    "            expected_min_start = prev_end\n",
    "            \n",
    "            # Ensure we don't search before previous aya ended\n",
    "            if global_whisper_pos < len(whisper_words):\n",
    "                while (global_whisper_pos < len(whisper_words) and \n",
    "                       whisper_words[global_whisper_pos][\"start_ms\"] < expected_min_start):\n",
    "                    global_whisper_pos += 1\n",
    "        \n",
    "        # STEP 1: Find anchor point for this aya\n",
    "        anchor_pos, anchor_confidence = find_aya_anchor(\n",
    "            aya_words, \n",
    "            whisper_words, \n",
    "            global_whisper_pos,\n",
    "            search_window=20\n",
    "        )\n",
    "        \n",
    "        # STEP 2: Align words from anchor\n",
    "        aligned_words, word_confidence, next_pos = align_words_with_confidence(\n",
    "            aya_words,\n",
    "            whisper_words,\n",
    "            anchor_pos\n",
    "        )\n",
    "        \n",
    "        # STEP 3: Fill any missing words\n",
    "        aligned_words = fill_missing_words(\n",
    "            aya_words,\n",
    "            aligned_words,\n",
    "            whisper_words,\n",
    "            next_pos\n",
    "        )\n",
    "        \n",
    "        # STEP 4: Calculate overall aya confidence\n",
    "        word_confidences = [w[\"confidence\"] for w in aligned_words]\n",
    "        aya_confidence = sum(word_confidences) / len(word_confidences) if word_confidences else 0.0\n",
    "        \n",
    "        # Penalize if we had to use gap filling\n",
    "        gap_filled_count = sum(1 for w in aligned_words if not w.get(\"matched\", False))\n",
    "        if gap_filled_count > 0:\n",
    "            gap_penalty = gap_filled_count / len(aligned_words)\n",
    "            aya_confidence *= (1.0 - (gap_penalty * 0.3))  # Reduce by up to 30%\n",
    "        \n",
    "        # STEP 5: Validate alignment quality\n",
    "        if len(aligned_words) != len(aya_words):\n",
    "            print(f\"  ‚ö†Ô∏è  Aya {aya_num}: Word count mismatch! \" +\n",
    "                  f\"Expected {len(aya_words)}, got {len(aligned_words)}\")\n",
    "            # Force correct count by trimming or padding\n",
    "            if len(aligned_words) > len(aya_words):\n",
    "                aligned_words = aligned_words[:len(aya_words)]\n",
    "            else:\n",
    "                # Should not happen after gap filling, but add safety\n",
    "                while len(aligned_words) < len(aya_words):\n",
    "                    last_end = aligned_words[-1][\"end_ms\"] if aligned_words else 0\n",
    "                    aligned_words.append({\n",
    "                        \"word\": aya_words[len(aligned_words)],\n",
    "                        \"start_ms\": last_end,\n",
    "                        \"end_ms\": last_end + 500,\n",
    "                        \"confidence\": 0.10,\n",
    "                        \"matched\": False\n",
    "                    })\n",
    "        \n",
    "        # STEP 5.5: Lookahead validation (check next aya aligns properly)\n",
    "        lookahead_confidence = 1.0\n",
    "        if aya_num < len(clean_lines):  # Not the last aya\n",
    "            next_aya_words = clean_lines[aya_num].split() if aya_num < len(clean_lines) else []\n",
    "            if next_aya_words and next_pos < len(whisper_words):\n",
    "                # Quick check: does next aya's first word appear soon?\n",
    "                next_context = \" \".join(normalize(w) for w in next_aya_words[:2])\n",
    "                found_next = False\n",
    "                for peek_pos in range(next_pos, min(next_pos + 10, len(whisper_words))):\n",
    "                    peek_context = \" \".join(\n",
    "                        normalize(whisper_words[peek_pos + j][\"word\"]) \n",
    "                        for j in range(min(2, len(whisper_words) - peek_pos))\n",
    "                    )\n",
    "                    if fuzz.ratio(peek_context, next_context) > 75:\n",
    "                        found_next = True\n",
    "                        break\n",
    "                \n",
    "                if not found_next:\n",
    "                    lookahead_confidence = 0.7  # Penalty if next aya doesn't align\n",
    "                    print(f\"  ‚ö†Ô∏è  Aya {aya_num}: Next aya boundary uncertain\")\n",
    "        \n",
    "        # Apply lookahead penalty to aya confidence\n",
    "        aya_confidence *= lookahead_confidence\n",
    "        \n",
    "        # STEP 6: Create aya record\n",
    "        aya_record = {\n",
    "            \"aya_number\": aya_num,\n",
    "            \"text\": aya_text,\n",
    "            \"words\": aligned_words,\n",
    "            \"start_ms\": aligned_words[0][\"start_ms\"] if aligned_words else 0,\n",
    "            \"end_ms\": aligned_words[-1][\"end_ms\"] if aligned_words else 0,\n",
    "            \"confidence\": round(aya_confidence, 4),\n",
    "            \"word_count\": len(aligned_words),\n",
    "            \"matched_words\": sum(1 for w in aligned_words if w.get(\"matched\", False)),\n",
    "            \"gap_filled_words\": gap_filled_count\n",
    "        }\n",
    "        \n",
    "        ayas.append(aya_record)\n",
    "        \n",
    "        # Track low confidence for review\n",
    "        if aya_confidence < 0.7:\n",
    "            low_confidence_ayas.append(aya_num)\n",
    "        \n",
    "        # STEP 7: Update global position\n",
    "        global_whisper_pos = next_pos\n",
    "    \n",
    "    # ========================================================================\n",
    "    # SAVE ALIGNED DATA\n",
    "    # ========================================================================\n",
    "    output_data = {\n",
    "        \"metadata\": {\n",
    "            **metadata,\n",
    "            \"alignment_version\": \"v2_robust\",\n",
    "            \"total_ayas\": len(ayas),\n",
    "            \"low_confidence_count\": len(low_confidence_ayas),\n",
    "            \"average_confidence\": round(\n",
    "                sum(a[\"confidence\"] for a in ayas) / len(ayas) if ayas else 0.0, \n",
    "                4\n",
    "            )\n",
    "        },\n",
    "        \"ayas\": ayas,\n",
    "        \"low_confidence_ayas\": low_confidence_ayas\n",
    "    }\n",
    "    \n",
    "    with open(aligned_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(output_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # Print summary\n",
    "    avg_conf = output_data[\"metadata\"][\"average_confidence\"]\n",
    "    print(f\"    ‚úì {len(ayas)} ayas aligned | Avg confidence: {avg_conf:.2%}\")\n",
    "    \n",
    "    if low_confidence_ayas:\n",
    "        print(f\"    ‚ö†Ô∏è  {len(low_confidence_ayas)} ayas need review: {low_confidence_ayas[:5]}\" + \n",
    "              (\"...\" if len(low_confidence_ayas) > 5 else \"\"))\n",
    "\n",
    "# ============================================================================\n",
    "# CLEANUP\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"CLEANING UP...\")\n",
    "print('='*70)\n",
    "\n",
    "deleted_count = 0\n",
    "for delete_json in Path(\".\").rglob(\"*_DELETE.json\"):\n",
    "    delete_json.unlink()\n",
    "    print(f\"‚úì Deleted {delete_json.name}\")\n",
    "    deleted_count += 1\n",
    "\n",
    "print(f\"\\n‚úì Removed {deleted_count} temporary _DELETE.json files\")\n",
    "print('='*70)\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================================\n",
    "print(f\"\\nALIGNMENT COMPLETE\")\n",
    "print(f\"   Already aligned  : {aligned}\")\n",
    "print(f\"   Newly aligned    : {processed}\")\n",
    "print(f\"   Total processed  : {aligned + processed}\")\n",
    "print('='*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c70a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 6.5 ‚Äî AUDIO VERIFICATION & RE-ALIGNMENT (LITERAL WHISPER CHECK)\n",
    "\n",
    "\"\"\"\n",
    "This cell takes low-confidence aligned ayas and LITERALLY re-transcribes \n",
    "the audio segment using Whisper to verify/correct the alignment.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "import whisper\n",
    "from pathlib import Path\n",
    "import json\n",
    "from pydub import AudioSegment\n",
    "import tempfile\n",
    "from rapidfuzz import fuzz\n",
    "import re\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIG\n",
    "# ============================================================================\n",
    "CONFIDENCE_THRESHOLD = 0.7  # Re-verify ayas below this\n",
    "MIN_AYA_DURATION_MS = 1000  # Skip very short segments (< 1 second)\n",
    "VERIFICATION_MODEL = \"base\"  # Fast model for verification (base/small/medium)\n",
    "\n",
    "# ============================================================================\n",
    "# SETUP\n",
    "# ============================================================================\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"AUDIO VERIFICATION ‚Äî LITERAL WHISPER RE-TRANSCRIPTION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Confidence threshold: {CONFIDENCE_THRESHOLD}\")\n",
    "print(f\"Verification model: {VERIFICATION_MODEL}\")\n",
    "print()\n",
    "\n",
    "# Load Whisper model\n",
    "print(\"Loading Whisper model...\")\n",
    "model = whisper.load_model(VERIFICATION_MODEL, device=\"cuda\")\n",
    "print(\"‚úì Model loaded\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "def normalize(text: str) -> str:\n",
    "    \"\"\"Normalize Arabic text\"\"\"\n",
    "    text = re.sub(r'[\\u064B-\\u0652\\u0670]', '', text)\n",
    "    text = re.sub(r'[ÿ£ÿ•ÿ¢]', 'ÿß', text)\n",
    "    text = re.sub(r'ÿ©', 'Ÿá', text)\n",
    "    text = re.sub(r'Ÿâ', 'Ÿä', text)\n",
    "    return text.strip()\n",
    "\n",
    "def extract_audio_segment(audio_path: Path, start_ms: int, end_ms: int) -> Path:\n",
    "    \"\"\"Extract a segment from audio file\"\"\"\n",
    "    audio = AudioSegment.from_file(str(audio_path))\n",
    "    segment = audio[start_ms:end_ms]\n",
    "    \n",
    "    # Save to temp file\n",
    "    temp_file = tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False)\n",
    "    segment.export(temp_file.name, format=\"wav\")\n",
    "    \n",
    "    return Path(temp_file.name)\n",
    "\n",
    "def transcribe_segment(audio_path: Path) -> dict:\n",
    "    \"\"\"Transcribe audio segment with Whisper\"\"\"\n",
    "    result = model.transcribe(\n",
    "        str(audio_path),\n",
    "        language=\"ar\",\n",
    "        word_timestamps=True,\n",
    "        verbose=False\n",
    "    )\n",
    "    return result\n",
    "\n",
    "def compare_transcriptions(expected: str, actual: str) -> float:\n",
    "    \"\"\"Compare expected vs actual transcription, return similarity score\"\"\"\n",
    "    expected_norm = normalize(expected)\n",
    "    actual_norm = normalize(actual)\n",
    "    \n",
    "    # Use token sort ratio (handles word order differences)\n",
    "    return fuzz.token_sort_ratio(expected_norm, actual_norm) / 100.0\n",
    "\n",
    "def realign_aya_words(\n",
    "    aya_words: list,\n",
    "    whisper_result: dict,\n",
    "    original_start_ms: int\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Re-align aya words using fresh Whisper transcription.\n",
    "    Returns new aligned words with updated timestamps.\n",
    "    \"\"\"\n",
    "    whisper_words = []\n",
    "    for seg in whisper_result.get(\"segments\", []):\n",
    "        for w in seg.get(\"words\", []):\n",
    "            whisper_words.append({\n",
    "                \"word\": w[\"word\"].strip(),\n",
    "                \"start_ms\": int(w[\"start\"] * 1000) + original_start_ms,  # Offset to absolute time\n",
    "                \"end_ms\": int(w[\"end\"] * 1000) + original_start_ms,\n",
    "                \"confidence\": round(w.get(\"probability\", 0.95), 4)\n",
    "            })\n",
    "    \n",
    "    # Align aya words to new whisper words\n",
    "    aligned = []\n",
    "    whisper_pos = 0\n",
    "    \n",
    "    for quran_word in aya_words:\n",
    "        if whisper_pos >= len(whisper_words):\n",
    "            # Out of whisper words - use last known time + estimate\n",
    "            if aligned:\n",
    "                last_end = aligned[-1][\"end_ms\"]\n",
    "                aligned.append({\n",
    "                    \"word\": quran_word,\n",
    "                    \"start_ms\": last_end,\n",
    "                    \"end_ms\": last_end + 500,\n",
    "                    \"confidence\": 0.30,\n",
    "                    \"matched\": False,\n",
    "                    \"verified\": True\n",
    "                })\n",
    "            continue\n",
    "        \n",
    "        whisper_word = whisper_words[whisper_pos]\n",
    "        quran_norm = normalize(quran_word)\n",
    "        whisper_norm = normalize(whisper_word[\"word\"])\n",
    "        \n",
    "        # Calculate match confidence\n",
    "        match_score = fuzz.ratio(quran_norm, whisper_norm) / 100.0\n",
    "        combined_confidence = (match_score * 0.7) + (whisper_word[\"confidence\"] * 0.3)\n",
    "        \n",
    "        aligned.append({\n",
    "            \"word\": quran_word,\n",
    "            \"start_ms\": whisper_word[\"start_ms\"],\n",
    "            \"end_ms\": whisper_word[\"end_ms\"],\n",
    "            \"confidence\": round(combined_confidence, 4),\n",
    "            \"matched\": match_score > 0.6,\n",
    "            \"verified\": True  # Flag that this was verified\n",
    "        })\n",
    "        \n",
    "        whisper_pos += 1\n",
    "    \n",
    "    # Fill any remaining words\n",
    "    while len(aligned) < len(aya_words):\n",
    "        if aligned:\n",
    "            last_end = aligned[-1][\"end_ms\"]\n",
    "            aligned.append({\n",
    "                \"word\": aya_words[len(aligned)],\n",
    "                \"start_ms\": last_end,\n",
    "                \"end_ms\": last_end + 500,\n",
    "                \"confidence\": 0.30,\n",
    "                \"matched\": False,\n",
    "                \"verified\": True\n",
    "            })\n",
    "        else:\n",
    "            aligned.append({\n",
    "                \"word\": aya_words[len(aligned)],\n",
    "                \"start_ms\": original_start_ms,\n",
    "                \"end_ms\": original_start_ms + 500,\n",
    "                \"confidence\": 0.30,\n",
    "                \"matched\": False,\n",
    "                \"verified\": True\n",
    "            })\n",
    "    \n",
    "    return aligned\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN VERIFICATION LOOP\n",
    "# ============================================================================\n",
    "verified_count = 0\n",
    "improved_count = 0\n",
    "skipped_count = 0\n",
    "failed_count = 0\n",
    "\n",
    "for aligned_json in sorted(Path(\".\").rglob(\"*_aligned.json\")):\n",
    "    print(f\"\\nProcessing: {aligned_json.name}\")\n",
    "    \n",
    "    # Load aligned data\n",
    "    try:\n",
    "        with open(aligned_json, encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error loading: {e}\")\n",
    "        failed_count += 1\n",
    "        continue\n",
    "    \n",
    "    # Find corresponding audio file\n",
    "    audio_path = aligned_json.parent / data[\"metadata\"][\"audio_file\"]\n",
    "    if not audio_path.exists():\n",
    "        print(f\"  ‚ùå Audio file not found: {audio_path.name}\")\n",
    "        failed_count += 1\n",
    "        continue\n",
    "    \n",
    "    # Find low-confidence ayas\n",
    "    ayas = data.get(\"ayas\", [])\n",
    "    low_conf_ayas = [a for a in ayas if a.get(\"confidence\", 1.0) < CONFIDENCE_THRESHOLD]\n",
    "    \n",
    "    if not low_conf_ayas:\n",
    "        print(f\"  ‚úì All ayas above threshold ({CONFIDENCE_THRESHOLD})\")\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "    \n",
    "    print(f\"  Found {len(low_conf_ayas)} low-confidence ayas to verify\")\n",
    "    \n",
    "    # Verify each low-confidence aya\n",
    "    changes_made = False\n",
    "    \n",
    "    for aya in low_conf_ayas:\n",
    "        aya_num = aya[\"aya_number\"]\n",
    "        aya_text = aya[\"text\"]\n",
    "        aya_words = aya_text.split()\n",
    "        start_ms = aya[\"start_ms\"]\n",
    "        end_ms = aya[\"end_ms\"]\n",
    "        duration_ms = end_ms - start_ms\n",
    "        old_confidence = aya[\"confidence\"]\n",
    "        \n",
    "        # Skip very short segments\n",
    "        if duration_ms < MIN_AYA_DURATION_MS:\n",
    "            print(f\"    Aya {aya_num}: Too short ({duration_ms}ms), skipping\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"    Aya {aya_num}: Conf={old_confidence:.2%}, Duration={duration_ms/1000:.1f}s\")\n",
    "        \n",
    "        try:\n",
    "            # STEP 1: Extract audio segment\n",
    "            temp_audio = extract_audio_segment(audio_path, start_ms, end_ms)\n",
    "            \n",
    "            # STEP 2: Re-transcribe with Whisper\n",
    "            whisper_result = transcribe_segment(temp_audio)\n",
    "            actual_text = whisper_result.get(\"text\", \"\").strip()\n",
    "            \n",
    "            # STEP 3: Compare transcriptions\n",
    "            similarity = compare_transcriptions(aya_text, actual_text)\n",
    "            \n",
    "            print(f\"      Expected: {aya_text[:50]}...\")\n",
    "            print(f\"      Got:      {actual_text[:50]}...\")\n",
    "            print(f\"      Match:    {similarity:.2%}\")\n",
    "            \n",
    "            # STEP 4: Decide action based on similarity\n",
    "            if similarity < 0.5:\n",
    "                # Very different - major alignment issue\n",
    "                print(f\"      ‚ö†Ô∏è  MAJOR MISMATCH - Re-aligning...\")\n",
    "                \n",
    "                # Re-align words with new transcription\n",
    "                new_aligned_words = realign_aya_words(\n",
    "                    aya_words,\n",
    "                    whisper_result,\n",
    "                    start_ms\n",
    "                )\n",
    "                \n",
    "                # Calculate new confidence\n",
    "                new_confidences = [w[\"confidence\"] for w in new_aligned_words]\n",
    "                new_confidence = sum(new_confidences) / len(new_confidences)\n",
    "                \n",
    "                # Update aya\n",
    "                aya[\"words\"] = new_aligned_words\n",
    "                aya[\"confidence\"] = round(new_confidence, 4)\n",
    "                aya[\"end_ms\"] = new_aligned_words[-1][\"end_ms\"]\n",
    "                aya[\"verification_status\"] = \"realigned\"\n",
    "                aya[\"similarity_before\"] = round(similarity, 4)\n",
    "                \n",
    "                changes_made = True\n",
    "                improved_count += 1\n",
    "                \n",
    "                print(f\"      ‚úì Re-aligned: {old_confidence:.2%} ‚Üí {new_confidence:.2%}\")\n",
    "                \n",
    "            elif similarity < 0.8:\n",
    "                # Moderate match - boost confidence but keep timing\n",
    "                print(f\"      ‚ö†Ô∏è  Moderate match - adjusting confidence...\")\n",
    "                \n",
    "                # Boost confidence based on similarity\n",
    "                confidence_boost = similarity * 0.3  # Up to 30% boost\n",
    "                new_confidence = min(old_confidence + confidence_boost, 0.95)\n",
    "                \n",
    "                aya[\"confidence\"] = round(new_confidence, 4)\n",
    "                aya[\"verification_status\"] = \"confidence_adjusted\"\n",
    "                aya[\"similarity_score\"] = round(similarity, 4)\n",
    "                \n",
    "                changes_made = True\n",
    "                improved_count += 1\n",
    "                \n",
    "                print(f\"      ‚úì Adjusted: {old_confidence:.2%} ‚Üí {new_confidence:.2%}\")\n",
    "                \n",
    "            else:\n",
    "                # Good match - confirm alignment is correct\n",
    "                print(f\"      ‚úì VERIFIED - Alignment correct\")\n",
    "                \n",
    "                aya[\"verification_status\"] = \"verified_correct\"\n",
    "                aya[\"similarity_score\"] = round(similarity, 4)\n",
    "                changes_made = True\n",
    "            \n",
    "            # Cleanup temp file\n",
    "            temp_audio.unlink()\n",
    "            \n",
    "            # Clear CUDA cache\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"      ‚ùå Verification failed: {e}\")\n",
    "            aya[\"verification_status\"] = \"verification_failed\"\n",
    "            aya[\"error\"] = str(e)\n",
    "            continue\n",
    "    \n",
    "    # Save updated data if changes were made\n",
    "    if changes_made:\n",
    "        # Update metadata\n",
    "        data[\"metadata\"][\"verification_version\"] = \"v1_audio_check\"\n",
    "        data[\"metadata\"][\"verified_ayas\"] = sum(\n",
    "            1 for a in ayas if \"verification_status\" in a\n",
    "        )\n",
    "        \n",
    "        # Recalculate average confidence\n",
    "        confidences = [a[\"confidence\"] for a in ayas]\n",
    "        data[\"metadata\"][\"average_confidence\"] = round(\n",
    "            sum(confidences) / len(confidences), 4\n",
    "        )\n",
    "        \n",
    "        # Update low confidence list\n",
    "        data[\"low_confidence_ayas\"] = [\n",
    "            a[\"aya_number\"] for a in ayas \n",
    "            if a.get(\"confidence\", 1.0) < CONFIDENCE_THRESHOLD\n",
    "        ]\n",
    "        data[\"metadata\"][\"low_confidence_count\"] = len(data[\"low_confidence_ayas\"])\n",
    "        \n",
    "        # Save\n",
    "        with open(aligned_json, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        verified_count += 1\n",
    "        print(f\"  ‚úì Updated and saved\")\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VERIFICATION COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Files verified and updated: {verified_count}\")\n",
    "print(f\"Ayas improved/corrected:    {improved_count}\")\n",
    "print(f\"Files skipped (all good):   {skipped_count}\")\n",
    "print(f\"Files failed:               {failed_count}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Cleanup\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print(\"\\n‚úì GPU memory cleared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394380d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 6.5 enhancment ‚Äî AUDIO VERIFICATION WITH WHISPERX + FINE-TUNE\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "import whisperx\n",
    "from pathlib import Path\n",
    "import json\n",
    "from pydub import AudioSegment\n",
    "import tempfile\n",
    "from rapidfuzz import fuzz\n",
    "import re\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIG\n",
    "# ============================================================================\n",
    "CONFIDENCE_THRESHOLD = 0.7\n",
    "MIN_AYA_DURATION_MS = 1000\n",
    "DEVICE = \"cuda\"\n",
    "COMPUTE_TYPE = \"float16\"\n",
    "BATCH_SIZE = 16\n",
    "MODEL_NAME = \"tarteel-ai/whisper-base-ar-quran\"\n",
    "ALIGN_MODEL = \"WAV2VEC2_ASR_BASE_960H\"  # Or suitable AR model\n",
    "\n",
    "# ============================================================================\n",
    "# SETUP\n",
    "# ============================================================================\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print(\"=\" * 80)\n",
    "print(\"AUDIO VERIFICATION ‚Äî WHISPERX WITH QURAN FINE-TUNE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Threshold: {CONFIDENCE_THRESHOLD} | Model: {MODEL_NAME}\")\n",
    "\n",
    "# Load WhisperX model\n",
    "model = whisperx.load_model(MODEL_NAME, DEVICE, compute_type=COMPUTE_TYPE)\n",
    "\n",
    "# Load alignment model (auto for AR)\n",
    "align_model, align_metadata = whisperx.load_align_model(language_code=\"ar\", device=DEVICE)\n",
    "\n",
    "print(\"‚úì Models loaded\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# HELPERS\n",
    "# ============================================================================\n",
    "def normalize(text: str) -> str:\n",
    "    text = re.sub(r'[\\u064B-\\u0652\\u0670]', '', text)\n",
    "    text = re.sub(r'[ÿ£ÿ•ÿ¢]', 'ÿß', text)\n",
    "    text = re.sub(r'ÿ©', 'Ÿá', text)\n",
    "    text = re.sub(r'Ÿâ', 'Ÿä', text)\n",
    "    return text.strip()\n",
    "\n",
    "def extract_audio_segment(audio_path: Path, start_ms: int, end_ms: int) -> AudioSegment:\n",
    "    audio = AudioSegment.from_file(str(audio_path))\n",
    "    return audio[start_ms:end_ms]\n",
    "\n",
    "def transcribe_segment(audio: AudioSegment) -> dict:\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".wav\") as temp_file:\n",
    "        audio.export(temp_file.name, format=\"wav\")\n",
    "        audio_data = whisperx.load_audio(temp_file.name)\n",
    "        \n",
    "        # Transcribe\n",
    "        result = model.transcribe(\n",
    "            audio_data,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            language=\"ar\"\n",
    "        )\n",
    "        \n",
    "        # Align\n",
    "        aligned = whisperx.align(\n",
    "            result[\"segments\"],\n",
    "            align_model,\n",
    "            align_metadata,\n",
    "            audio_data,\n",
    "            DEVICE,\n",
    "            return_char_alignments=False\n",
    "        )\n",
    "        \n",
    "    return aligned\n",
    "\n",
    "def compare_transcriptions(expected: str, actual: str) -> float:\n",
    "    return fuzz.token_sort_ratio(normalize(expected), normalize(actual)) / 100.0\n",
    "\n",
    "def realign_aya_words(\n",
    "    aya_words: list,\n",
    "    whisper_result: dict,\n",
    "    original_start_ms: int\n",
    ") -> list:\n",
    "    whisper_words = []\n",
    "    for seg in whisper_result.get(\"segments\", []):\n",
    "        for w in seg.get(\"words\", []):\n",
    "            whisper_words.append({\n",
    "                \"word\": w[\"word\"].strip(),\n",
    "                \"start_ms\": int(w[\"start\"] * 1000) + original_start_ms,\n",
    "                \"end_ms\": int(w[\"end\"] * 1000) + original_start_ms,\n",
    "                \"confidence\": round(w.get(\"probability\", 0.95), 4)\n",
    "            })\n",
    "    \n",
    "    aligned = []\n",
    "    whisper_pos = 0\n",
    "    \n",
    "    for quran_word in aya_words:\n",
    "        if whisper_pos >= len(whisper_words):\n",
    "            if aligned:\n",
    "                last_end = aligned[-1][\"end_ms\"]\n",
    "                aligned.append({\n",
    "                    \"word\": quran_word,\n",
    "                    \"start_ms\": last_end,\n",
    "                    \"end_ms\": last_end + 500,\n",
    "                    \"confidence\": 0.30,\n",
    "                    \"matched\": False,\n",
    "                    \"verified\": True\n",
    "                })\n",
    "            continue\n",
    "        \n",
    "        whisper_word = whisper_words[whisper_pos]\n",
    "        match_score = fuzz.ratio(normalize(quran_word), normalize(whisper_word[\"word\"])) / 100.0\n",
    "        combined_conf = (match_score * 0.7) + (whisper_word[\"confidence\"] * 0.3)\n",
    "        \n",
    "        aligned.append({\n",
    "            \"word\": quran_word,\n",
    "            \"start_ms\": whisper_word[\"start_ms\"],\n",
    "            \"end_ms\": whisper_word[\"end_ms\"],\n",
    "            \"confidence\": round(combined_conf, 4),\n",
    "            \"matched\": match_score > 0.6,\n",
    "            \"verified\": True\n",
    "        })\n",
    "        \n",
    "        whisper_pos += 1\n",
    "    \n",
    "    while len(aligned) < len(aya_words):\n",
    "        last_end = aligned[-1][\"end_ms\"] if aligned else original_start_ms\n",
    "        aligned.append({\n",
    "            \"word\": aya_words[len(aligned)],\n",
    "            \"start_ms\": last_end,\n",
    "            \"end_ms\": last_end + 500,\n",
    "            \"confidence\": 0.30,\n",
    "            \"matched\": False,\n",
    "            \"verified\": True\n",
    "        })\n",
    "    \n",
    "    return aligned\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN LOOP\n",
    "# ============================================================================\n",
    "verified_count = improved_count = skipped_count = failed_count = 0\n",
    "\n",
    "for aligned_json in sorted(Path(\".\").rglob(\"*_aligned.json\")):\n",
    "    print(f\"\\nProcessing: {aligned_json.name}\")\n",
    "    \n",
    "    with open(aligned_json, encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    audio_path = aligned_json.parent / data[\"metadata\"][\"audio_file\"]\n",
    "    if not audio_path.exists():\n",
    "        print(f\" ‚ùå Audio not found\")\n",
    "        failed_count += 1\n",
    "        continue\n",
    "    \n",
    "    ayas = data.get(\"ayas\", [])\n",
    "    low_conf_ayas = [a for a in ayas if a.get(\"confidence\", 1.0) < CONFIDENCE_THRESHOLD]\n",
    "    \n",
    "    if not low_conf_ayas:\n",
    "        print(\" ‚úì All good\")\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "    \n",
    "    print(f\" Verifying {len(low_conf_ayas)} ayas\")\n",
    "    changes_made = False\n",
    "    \n",
    "    for aya in low_conf_ayas:\n",
    "        aya_num = aya[\"aya_number\"]\n",
    "        aya_text = aya[\"text\"]\n",
    "        aya_words = aya_text.split()\n",
    "        start_ms = aya[\"start_ms\"]\n",
    "        end_ms = aya[\"end_ms\"]\n",
    "        duration_ms = end_ms - start_ms\n",
    "        old_conf = aya[\"confidence\"]\n",
    "        \n",
    "        if duration_ms < MIN_AYA_DURATION_MS:\n",
    "            continue\n",
    "        \n",
    "        print(f\" Aya {aya_num}: Conf={old_conf:.2%}, Dur={duration_ms/1000:.1f}s\")\n",
    "        \n",
    "        try:\n",
    "            audio_seg = extract_audio_segment(audio_path, start_ms, end_ms)\n",
    "            whisper_result = transcribe_segment(audio_seg)\n",
    "            actual_text = whisper_result.get(\"text\", \"\").strip()\n",
    "            \n",
    "            similarity = compare_transcriptions(aya_text, actual_text)\n",
    "            print(f\" Match: {similarity:.2%}\")\n",
    "            \n",
    "            if similarity < 0.5:\n",
    "                new_words = realign_aya_words(aya_words, whisper_result, start_ms)\n",
    "                new_confs = [w[\"confidence\"] for w in new_words]\n",
    "                new_conf = sum(new_confs) / len(new_confs)\n",
    "                \n",
    "                aya[\"words\"] = new_words\n",
    "                aya[\"confidence\"] = round(new_conf, 4)\n",
    "                aya[\"end_ms\"] = new_words[-1][\"end_ms\"]\n",
    "                aya[\"verification_status\"] = \"realigned\"\n",
    "                aya[\"similarity_before\"] = round(similarity, 4)\n",
    "                \n",
    "                changes_made = True\n",
    "                improved_count += 1\n",
    "                print(f\" ‚úì Realigned: {old_conf:.2%} ‚Üí {new_conf:.2%}\")\n",
    "            \n",
    "            elif similarity < 0.8:\n",
    "                boost = similarity * 0.3\n",
    "                new_conf = min(old_conf + boost, 0.95)\n",
    "                \n",
    "                aya[\"confidence\"] = round(new_conf, 4)\n",
    "                aya[\"verification_status\"] = \"adjusted\"\n",
    "                aya[\"similarity_score\"] = round(similarity, 4)\n",
    "                \n",
    "                changes_made = True\n",
    "                improved_count += 1\n",
    "                print(f\" ‚úì Adjusted: {old_conf:.2%} ‚Üí {new_conf:.2%}\")\n",
    "            \n",
    "            else:\n",
    "                aya[\"verification_status\"] = \"verified\"\n",
    "                aya[\"similarity_score\"] = round(similarity, 4)\n",
    "                changes_made = True\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\" ‚ùå Failed: {e}\")\n",
    "            aya[\"verification_status\"] = \"failed\"\n",
    "            aya[\"error\"] = str(e)\n",
    "            continue\n",
    "    \n",
    "    if changes_made:\n",
    "        data[\"metadata\"][\"verification_version\"] = \"whisperx_quran\"\n",
    "        data[\"metadata\"][\"verified_ayas\"] = sum(1 for a in ayas if \"verification_status\" in a)\n",
    "        \n",
    "        confs = [a[\"confidence\"] for a in ayas]\n",
    "        data[\"metadata\"][\"average_confidence\"] = round(sum(confs) / len(confs), 4)\n",
    "        \n",
    "        data[\"low_confidence_ayas\"] = [a[\"aya_number\"] for a in ayas if a.get(\"confidence\", 1.0) < CONFIDENCE_THRESHOLD]\n",
    "        data[\"metadata\"][\"low_confidence_count\"] = len(data[\"low_confidence_ayas\"])\n",
    "        \n",
    "        with open(aligned_json, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        verified_count += 1\n",
    "        print(\" ‚úì Saved\")\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"Verified: {verified_count} | Improved: {improved_count} | Skipped: {skipped_count} | Failed: {failed_count}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Cleanup\n",
    "del model, align_model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print(\"‚úì GPU cleared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe267d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 7 ‚Äî Generate HTML Viewer\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "for aligned_json in Path(\".\").rglob(\"*_aligned.json\"):\n",
    "    # Find corresponding audio file\n",
    "    base = aligned_json.stem.replace(\"_aligned\", \"\")\n",
    "    audio_file = next(aligned_json.parent.glob(f\"{base}.*\"), None)\n",
    "    \n",
    "    if not audio_file or audio_file.suffix.lower() not in ['.mp3', '.wav', '.m4a']:\n",
    "        continue\n",
    "    \n",
    "    with open(aligned_json, encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    html = f\"\"\"<!DOCTYPE html>\n",
    "<html dir=\"rtl\" lang=\"ar\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>{base}</title>\n",
    "    <style>\n",
    "        * {{ margin: 0; padding: 0; box-sizing: border-box; }}\n",
    "        body {{\n",
    "            font-family: 'Amiri', 'Scheherazade', serif;\n",
    "            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "            min-height: 100vh;\n",
    "            padding: 2rem;\n",
    "        }}\n",
    "        .container {{\n",
    "            max-width: 900px;\n",
    "            margin: 0 auto;\n",
    "            background: white;\n",
    "            border-radius: 16px;\n",
    "            box-shadow: 0 20px 60px rgba(0,0,0,0.3);\n",
    "            overflow: hidden;\n",
    "        }}\n",
    "        .header {{\n",
    "            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "            color: white;\n",
    "            padding: 2rem;\n",
    "            text-align: center;\n",
    "        }}\n",
    "        .header h1 {{ font-size: 2rem; margin-bottom: 0.5rem; }}\n",
    "        .audio-player {{\n",
    "            padding: 1.5rem;\n",
    "            background: #f8f9fa;\n",
    "            border-bottom: 1px solid #e9ecef;\n",
    "        }}\n",
    "        audio {{\n",
    "            width: 100%;\n",
    "            height: 40px;\n",
    "        }}\n",
    "        .ayas {{\n",
    "            padding: 2rem;\n",
    "        }}\n",
    "        .aya {{\n",
    "            font-size: 1.8rem;\n",
    "            line-height: 3rem;\n",
    "            margin-bottom: 2rem;\n",
    "            padding: 1.5rem;\n",
    "            border-radius: 12px;\n",
    "            transition: all 0.3s ease;\n",
    "            cursor: pointer;\n",
    "        }}\n",
    "        .aya:hover {{\n",
    "            background: #f8f9fa;\n",
    "        }}\n",
    "        .aya.active {{\n",
    "            background: #667eea;\n",
    "            color: white;\n",
    "            box-shadow: 0 4px 12px rgba(102, 126, 234, 0.4);\n",
    "        }}\n",
    "        .word {{\n",
    "            display: inline-block;\n",
    "            padding: 0.2rem 0.4rem;\n",
    "            margin: 0 0.2rem;\n",
    "            border-radius: 6px;\n",
    "            transition: all 0.2s ease;\n",
    "        }}\n",
    "        .aya.active .word.highlight {{\n",
    "            background: rgba(255, 255, 255, 0.3);\n",
    "            transform: scale(1.05);\n",
    "        }}\n",
    "        .aya-number {{\n",
    "            display: inline-block;\n",
    "            width: 2rem;\n",
    "            height: 2rem;\n",
    "            line-height: 2rem;\n",
    "            text-align: center;\n",
    "            background: #667eea;\n",
    "            color: white;\n",
    "            border-radius: 50%;\n",
    "            font-size: 1rem;\n",
    "            margin-left: 0.5rem;\n",
    "        }}\n",
    "        .aya.active .aya-number {{\n",
    "            background: white;\n",
    "            color: #667eea;\n",
    "        }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <div class=\"header\">\n",
    "            <h1>{base.replace('_', ' ').title()}</h1>\n",
    "        </div>\n",
    "        <div class=\"audio-player\">\n",
    "            <audio id=\"audio\" controls>\n",
    "                <source src=\"{audio_file.name}\" type=\"audio/{audio_file.suffix[1:]}\">\n",
    "            </audio>\n",
    "        </div>\n",
    "        <div class=\"ayas\">\n",
    "\"\"\"\n",
    "    \n",
    "    for aya in data['ayas']:\n",
    "        words_html = \"\"\n",
    "        for w in aya['words']:\n",
    "            words_html += f'<span class=\"word\" data-start=\"{w[\"start_ms\"]}\" data-end=\"{w[\"end_ms\"]}\">{w[\"word\"]}</span>'\n",
    "        \n",
    "        html += f'''            <div class=\"aya\" data-start=\"{aya['start_ms']}\" data-end=\"{aya['end_ms']}\">\n",
    "                <span class=\"aya-number\">{aya['aya_number']}</span>\n",
    "                {words_html}\n",
    "            </div>\n",
    "'''\n",
    "    \n",
    "    html += \"\"\"        </div>\n",
    "    </div>\n",
    "    <script>\n",
    "        const audio = document.getElementById('audio');\n",
    "        const ayas = document.querySelectorAll('.aya');\n",
    "        \n",
    "        // Click aya to play\n",
    "        ayas.forEach(aya => {\n",
    "            aya.addEventListener('click', () => {\n",
    "                const start = parseInt(aya.dataset.start);\n",
    "                audio.currentTime = start / 1000;\n",
    "                audio.play();\n",
    "            });\n",
    "        });\n",
    "        \n",
    "        // Highlight during playback\n",
    "        audio.addEventListener('timeupdate', () => {\n",
    "            const currentMs = audio.currentTime * 1000;\n",
    "            \n",
    "            ayas.forEach(aya => {\n",
    "                const start = parseInt(aya.dataset.start);\n",
    "                const end = parseInt(aya.dataset.end);\n",
    "                \n",
    "                if (currentMs >= start && currentMs <= end) {\n",
    "                    aya.classList.add('active');\n",
    "                    \n",
    "                    // Highlight words\n",
    "                    const words = aya.querySelectorAll('.word');\n",
    "                    words.forEach(word => {\n",
    "                        const wStart = parseInt(word.dataset.start);\n",
    "                        const wEnd = parseInt(word.dataset.end);\n",
    "                        \n",
    "                        if (currentMs >= wStart && currentMs <= wEnd) {\n",
    "                            word.classList.add('highlight');\n",
    "                        } else {\n",
    "                            word.classList.remove('highlight');\n",
    "                        }\n",
    "                    });\n",
    "                } else {\n",
    "                    aya.classList.remove('active');\n",
    "                    aya.querySelectorAll('.word').forEach(w => w.classList.remove('highlight'));\n",
    "                }\n",
    "            });\n",
    "        });\n",
    "    </script>\n",
    "</body>\n",
    "</html>\"\"\"\n",
    "    \n",
    "    html_file = aligned_json.with_suffix('.html')\n",
    "    with open(html_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(html)\n",
    "    \n",
    "    print(f\"‚úì {html_file.name}\")\n",
    "\n",
    "print(\"\\n‚úÖ HTML viewers created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf6a140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleted_count = 0\n",
    "# for delete_json in Path(\".\").rglob(\"*_DELETE.json\"):\n",
    "#     delete_json.unlink()\n",
    "#     print(f\"‚úì Deleted {delete_json.name}\")\n",
    "#     deleted_count += 1\n",
    "\n",
    "# print(f\"\\n‚úì Removed {deleted_count} temporary _DELETE.json files\")\n",
    "# print('='*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f5cb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 8 ‚Äî FastAPI Backend for Quran Alignment Data\n",
    "\n",
    "import json\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "\n",
    "app = FastAPI(title=\"Quran Alignment API\", version=\"1.0.0\")\n",
    "\n",
    "# Add CORS middleware\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "class Word(BaseModel):\n",
    "    word: str\n",
    "    start_ms: int\n",
    "    end_ms: int\n",
    "\n",
    "class Aya(BaseModel):\n",
    "    aya_number: int\n",
    "    text: str\n",
    "    words: List[Word]\n",
    "    start_ms: int\n",
    "    end_ms: int\n",
    "\n",
    "class QuranAlignment(BaseModel):\n",
    "    ayas: List[Aya]\n",
    "\n",
    "# Load all aligned data at startup\n",
    "aligned_data = {}\n",
    "\n",
    "def load_aligned_files():\n",
    "    global aligned_data\n",
    "    aligned_data = {}\n",
    "    for json_file in Path(\".\").rglob(\"*_aligned.json\"):\n",
    "        try:\n",
    "            with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            surah_name = json_file.stem.replace('_aligned', '')\n",
    "            aligned_data[surah_name] = data\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {json_file}: {e}\")\n",
    "\n",
    "@app.on_event('startup')\n",
    "async def startup_event():\n",
    "    load_aligned_files()\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {\"message\": \"Quran Alignment API - Use /surahs to list available surahs\"}\n",
    "\n",
    "@app.get(\"/surahs\")\n",
    "async def get_surahs():\n",
    "    return {\"surahs\": list(aligned_data.keys())}\n",
    "\n",
    "@app.get(\"/surah/{surah_name}\")\n",
    "async def get_surah(surah_name: str):\n",
    "    if surah_name not in aligned_data:\n",
    "        raise HTTPException(status_code=404, detail=\"Surah not found\")\n",
    "    return aligned_data[surah_name]\n",
    "\n",
    "@app.get(\"/surah/{surah_name}/aya/{aya_number}\")\n",
    "async def get_aya(surah_name: str, aya_number: int):\n",
    "    if surah_name not in aligned_data:\n",
    "        raise HTTPException(status_code=404, detail=\"Surah not found\")\n",
    "    \n",
    "    surah_data = aligned_data[surah_name]\n",
    "    ayas = surah_data.get(\"ayas\", [])\n",
    "    \n",
    "    if 1 <= aya_number <= len(ayas):\n",
    "        return ayas[aya_number - 1]\n",
    "    else:\n",
    "        raise HTTPException(status_code=404, detail=\"Aya not found\")\n",
    "\n",
    "# Run with: uvicorn.run(app, host=\"127.0.0.1\", port=8000, reload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdb79ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 9 ‚Äî Beautiful Glassmorphism Dashboard with Streamlit\n",
    "\n",
    "import streamlit as st\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Set page config\n",
    "st.set_page_config(\n",
    "    page_title=\" Quran Alignment Dashboard\",\n",
    "    page_icon=\"üìñ\",\n",
    "    layout=\"wide\"\n",
    ")\n",
    "\n",
    "# Custom CSS for glassmorphism\n",
    "st.markdown(\"\"\"\n",
    "<style>\n",
    "    @import url('https://fonts.googleapis.com/css2?family=Amiri:wght@400;700&display=swap');\n",
    "    \n",
    "    .glass-card {\n",
    "        background: rgba(255, 255, 255, 0.15);\n",
    "        backdrop-filter: blur(10px);\n",
    "        border-radius: 16px;\n",
    "        border: 1px solid rgba(255, 255, 255, 0.18);\n",
    "        padding: 20px;\n",
    "        margin: 10px 0;\n",
    "        box-shadow: 0 8px 32px 0 rgba(31, 38, 135, 0.37);\n",
    "    }\n",
    "    \n",
    "    .aya-card {\n",
    "        background: rgba(255, 255, 255, 0.1);\n",
    "        border-radius: 12px;\n",
    "        padding: 15px;\n",
    "        margin: 10px 0;\n",
    "        border: 1px solid rgba(255, 255, 255, 0.1);\n",
    "    }\n",
    "    \n",
    "    .word-timestamp {\n",
    "        background: rgba(102, 126, 234, 0.2);\n",
    "        border-radius: 6px;\n",
    "        padding: 2px 6px;\n",
    "        margin: 0 2px;\n",
    "        font-size: 0.8em;\n",
    "    }\n",
    "    \n",
    "    body {\n",
    "        font-family: 'Amiri', 'Scheherazade', serif !important;\n",
    "    }\n",
    "    \n",
    "    .arabic-text {\n",
    "        font-size: 1.8rem !important;\n",
    "        line-height: 2.5rem !important;\n",
    "        text-align: right !important;\n",
    "        direction: rtl !important;\n",
    "    }\n",
    "</style>\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "st.markdown(\"<h1 style='text-align: center; color: white;'>üìñ Quran Alignment Dashboard</h1>\", unsafe_allow_html=True)\n",
    "\n",
    "# Load all aligned files\n",
    "aligned_files = list(Path(\".\").rglob(\"*_aligned.json\"))\n",
    "\n",
    "if not aligned_files:\n",
    "    st.error(\"‚ùå No aligned Quran files found! Run Cell 6 first.\")\n",
    "    st.stop()\n",
    "\n",
    "# Sidebar for selection\n",
    "with st.sidebar:\n",
    "    st.markdown('<div class=\"glass-card\">', unsafe_allow_html=True)\n",
    "    st.header(\"üìã Surah Selection\")\n",
    "    \n",
    "    # Create a mapping of display names to file paths\n",
    "    surah_options = {}\n",
    "    for f in aligned_files:\n",
    "        display_name = f.stem.replace('_aligned', '').replace('_', ' ').title()\n",
    "        surah_options[display_name] = f\n",
    "    \n",
    "    selected_surah = st.selectbox(\n",
    "        \"Choose a Surah:\",\n",
    "        options=list(surah_options.keys()),\n",
    "        format_func=lambda x: x\n",
    "    )\n",
    "    \n",
    "    selected_file = surah_options[selected_surah]\n",
    "    st.success(f\"Selected: {selected_file.name}\")\n",
    "    st.markdown('</div>', unsafe_allow_html=True)\n",
    "\n",
    "# Main content\n",
    "col1, col2 = st.columns([2, 1])\n",
    "\n",
    "with col1:\n",
    "    st.markdown(f'<div class=\"glass-card\"><h2>üìñ {selected_surah}</h2></div>', unsafe_allow_html=True)\n",
    "    \n",
    "    # Load and display the selected surah\n",
    "    with open(selected_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    ayas = data.get('ayas', [])\n",
    "    \n",
    "    for aya in ayas:\n",
    "        with st.container():\n",
    "            st.markdown(f'''\n",
    "            <div class=\"aya-card\">\n",
    "                <div style=\"display: flex; justify-content: space-between; align-items: center; margin-bottom: 10px;\">\n",
    "                    <span style=\"background: rgba(102, 126, 234, 0.3); padding: 4px 12px; border-radius: 20px; font-weight: bold;\">\n",
    "                        ÿ¢Ÿäÿ© {aya['aya_number']}\n",
    "                    </span>\n",
    "                    <span class=\"word-timestamp\">\n",
    "                        {aya['start_ms']//1000}s - {aya['end_ms']//1000}s\n",
    "                    </span>\n",
    "                </div>\n",
    "                <div class=\"arabic-text\">\n",
    "                    {aya['text']}\n",
    "                </div>\n",
    "            </div>\n",
    "            ''', unsafe_allow_html=True)\n",
    "            \n",
    "            # Show word-by-word breakdown\n",
    "            if st.expander(f\"üîç Words for Aya {aya['aya_number']}\"):\n",
    "                words_html = \"\"\n",
    "                for word_data in aya['words']:\n",
    "                    words_html += f'''\n",
    "                    <span class=\"word-timestamp\" style=\"margin: 2px;\">\n",
    "                        {word_data['word']} ({word_data['start_ms']//1000}s-{word_data['end_ms']//1000}s)\n",
    "                    </span>\n",
    "                    '''\n",
    "                st.markdown(f'<div style=\"direction: rtl; text-align: right;\">{words_html}</div>', unsafe_allow_html=True)\n",
    "\n",
    "with col2:\n",
    "    st.markdown('<div class=\"glass-card\"><h3>üìä Statistics</h3></div>', unsafe_allow_html=True)\n",
    "    \n",
    "    total_ayas = len(ayas)\n",
    "    total_words = sum(len(aya['words']) for aya in ayas)\n",
    "    total_duration = sum(aya['end_ms'] - aya['start_ms'] for aya in ayas) / 1000  # in seconds\n",
    "    \n",
    "    st.metric(\"ayas\", total_ayas)\n",
    "    st.metric(\"Words\", total_words)\n",
    "    st.metric(\"Duration\", f\"{total_duration/60:.1f} min\")\n",
    "    \n",
    "    st.markdown('<div class=\"glass-card\"><h3>üìÅ Available Surahs</h3></div>', unsafe_allow_html=True)\n",
    "    for f in aligned_files:\n",
    "        name = f.stem.replace('_aligned', '').replace('_', ' ').title()\n",
    "        st.write(f\"üìÑ {name}\")\n",
    "\n",
    "st.markdown(\"<hr style='margin: 30px 0;'>\", unsafe_allow_html=True)\n",
    "st.markdown(\"<p style='text-align: center; color: rgba(255,255,255,0.6);'>‚ú® Quran Alignment Dashboard ‚ú®</p>\", unsafe_allow_html=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c2bf3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quran-py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
