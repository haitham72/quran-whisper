{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "QURAN WORD-BY-WORD ALIGNMENT\n",
      "============================================================\n",
      "\n",
      "=== SYSTEM CHECK ===\n",
      "GPU: NVIDIA GeForce RTX 4080\n",
      "Total GPU Memory: 15.99GB\n",
      "Currently Used: 0.00GB\n",
      "Reserved: 0.00GB\n",
      "Free: 15.99GB\n",
      "‚úì Sufficient GPU memory available\n",
      "Cleared CUDA cache\n",
      "\n",
      "=== FILE DETECTION ===\n",
      "‚úì Audio: bakara.mp3\n",
      "‚úì Text: baqara.txt\n",
      "\n",
      "Will create:\n",
      "  - bakara_whisper_raw.json\n",
      "  - bakara_timings.json\n",
      "\n",
      "Audio file size: 3.10MB\n",
      "Audio sample rate: 16000Hz\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# BLOCK 1: AUTO-DETECT FILES + GPU CHECK\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from rapidfuzz import fuzz\n",
    "import re\n",
    "import torch\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"QURAN WORD-BY-WORD ALIGNMENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# GPU Check\n",
    "print(\"\\n=== SYSTEM CHECK ===\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    total_mem = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "    reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "    free = total_mem - reserved\n",
    "    \n",
    "    print(f\"Total GPU Memory: {total_mem:.2f}GB\")\n",
    "    print(f\"Currently Used: {allocated:.2f}GB\")\n",
    "    print(f\"Reserved: {reserved:.2f}GB\")\n",
    "    print(f\"Free: {free:.2f}GB\")\n",
    "    \n",
    "    if free < 4.0:\n",
    "        print(\"‚ö† WARNING: Less than 4GB free! May need CPU mode.\")\n",
    "    else:\n",
    "        print(\"‚úì Sufficient GPU memory available\")\n",
    "    \n",
    "    # Clear any existing allocations\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Cleared CUDA cache\")\n",
    "else:\n",
    "    print(\"No GPU detected - will use CPU\")\n",
    "\n",
    "# Find files\n",
    "print(\"\\n=== FILE DETECTION ===\")\n",
    "audio_files = list(Path(\".\").glob(\"*.mp3\")) + list(Path(\".\").glob(\"*.wav\"))\n",
    "if not audio_files:\n",
    "    raise FileNotFoundError(\"No audio file found!\")\n",
    "audio_file = str(audio_files[0])\n",
    "\n",
    "text_files = list(Path(\".\").glob(\"*.txt\"))\n",
    "if not text_files:\n",
    "    raise FileNotFoundError(\"No text file found!\")\n",
    "text_file = str(text_files[0])\n",
    "\n",
    "base_name = Path(audio_file).stem\n",
    "output_json = f\"{base_name}_timings.json\"\n",
    "whisper_raw = f\"{base_name}_whisper_raw.json\"\n",
    "\n",
    "print(f\"‚úì Audio: {audio_file}\")\n",
    "print(f\"‚úì Text: {text_file}\")\n",
    "print(f\"\\nWill create:\")\n",
    "print(f\"  - {whisper_raw}\")\n",
    "print(f\"  - {output_json}\")\n",
    "\n",
    "# Check audio file size\n",
    "audio_size = os.path.getsize(audio_file) / 1024**2\n",
    "print(f\"\\nAudio file size: {audio_size:.2f}MB\")\n",
    "\n",
    "# Estimate processing requirements\n",
    "import librosa\n",
    "y, sr = librosa.load(audio_file, sr=16000, duration=1)  # Just load 1s to check\n",
    "print(f\"Audio sample rate: {sr}Hz\")\n",
    "del y  # Clean up\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 1: WHISPER TRANSCRIPTION\n",
      "============================================================\n",
      "\n",
      "‚úì Using existing (APPROVED): bakara_whisper_raw.json\n",
      "   (Delete this file to re-transcribe)\n",
      "\n",
      "üìä Whisper: 150 words\n"
     ]
    }
   ],
   "source": [
    "# BLOCK 2: WHISPER TRANSCRIPTION WITH PROGRESS & GPU MONITORING\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 1: WHISPER TRANSCRIPTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from faster_whisper import WhisperModel\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# GPU Monitoring function\n",
    "def print_gpu_usage():\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_mem = torch.cuda.memory_allocated() / 1024**3\n",
    "        gpu_max = torch.cuda.max_memory_allocated() / 1024**3\n",
    "        gpu_reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "        print(f\"  GPU Memory: {gpu_mem:.2f}GB used | {gpu_max:.2f}GB peak | {gpu_reserved:.2f}GB reserved\")\n",
    "    else:\n",
    "        print(\"  CPU mode (no GPU)\")\n",
    "\n",
    "if os.path.exists(whisper_raw):\n",
    "    print(f\"\\n‚úì Using existing (APPROVED): {whisper_raw}\")\n",
    "    print(\"   (Delete this file to re-transcribe)\")\n",
    "    with open(whisper_raw, 'r', encoding='utf-8') as f:\n",
    "        whisper_words = json.load(f)\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  {whisper_raw} not found - will transcribe\")\n",
    "    print(\"\\nLoading Whisper model...\")\n",
    "    \n",
    "    # Clear CUDA cache\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"Cleared CUDA cache\")\n",
    "    \n",
    "    print_gpu_usage()\n",
    "    \n",
    "    # Use smaller model or CPU if GPU memory is limited\n",
    "    try:\n",
    "        model = WhisperModel(\"large-v2\", device=\"cuda\", compute_type=\"float16\")\n",
    "        print(\"Loaded large-v2 model on GPU\")\n",
    "    except Exception as e:\n",
    "        print(f\"GPU failed ({e}), falling back to CPU...\")\n",
    "        model = WhisperModel(\"large-v2\", device=\"cpu\", compute_type=\"int8\")\n",
    "    \n",
    "    print_gpu_usage()\n",
    "    \n",
    "    print(f\"\\nTranscribing: {audio_file}\")\n",
    "    print(\"This may take several minutes...\")\n",
    "    \n",
    "    # Get audio duration for progress bar\n",
    "    import librosa\n",
    "    y, sr = librosa.load(audio_file, sr=16000)\n",
    "    duration = len(y) / sr\n",
    "    print(f\"Audio duration: {duration:.1f}s\")\n",
    "    \n",
    "    # Transcribe with progress tracking\n",
    "    segments_generator, info = model.transcribe(\n",
    "        audio_file,\n",
    "        language=\"ar\",\n",
    "        word_timestamps=True,\n",
    "        beam_size=5,\n",
    "        best_of=5,\n",
    "        temperature=0.0\n",
    "    )\n",
    "    \n",
    "    print(f\"Detected language: {info.language} (probability: {info.language_probability:.2f})\")\n",
    "    \n",
    "    # Process segments with progress bar\n",
    "    whisper_words = []\n",
    "    segment_count = 0\n",
    "    \n",
    "    with tqdm(total=int(duration), desc=\"Transcribing\", unit=\"s\") as pbar:\n",
    "        for segment in segments_generator:\n",
    "            segment_count += 1\n",
    "            for word in segment.words:\n",
    "                whisper_words.append({\n",
    "                    \"word\": word.word.strip(),\n",
    "                    \"start_ms\": int(word.start * 1000),\n",
    "                    \"end_ms\": int(word.end * 1000)\n",
    "                })\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.update(int(segment.end - segment.start))\n",
    "            \n",
    "            # Show GPU usage every 10 segments\n",
    "            if segment_count % 10 == 0:\n",
    "                print_gpu_usage()\n",
    "    \n",
    "    print(f\"\\nProcessed {segment_count} segments, found {len(whisper_words)} words\")\n",
    "    print_gpu_usage()\n",
    "    \n",
    "    # Save\n",
    "    with open(whisper_raw, 'w', encoding='utf-8') as f:\n",
    "        json.dump(whisper_words, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"‚úì Saved: {whisper_raw}\")\n",
    "    \n",
    "    # Clear GPU memory after transcription\n",
    "    if torch.cuda.is_available():\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"\\nCleared GPU memory\")\n",
    "        print_gpu_usage()\n",
    "\n",
    "print(f\"\\nüìä Whisper: {len(whisper_words)} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 2: ANCHOR-BASED ALIGNMENT\n",
      "============================================================\n",
      "‚úì Skipped Basmala\n",
      "\n",
      "Quran: 200 words | Whisper: 150 words\n",
      "\n",
      "üîç Finding all possible matches...\n",
      "\n",
      "‚öì Finding anchor points...\n",
      "Found 163 anchor points (‚â•80% confidence)\n",
      "  Anchor 0: Q[0]='ÿßŸÑŸìŸÖŸì' ‚Üî W[47] (score: 86%)\n",
      "  Anchor 1: Q[1]='ÿ∞ŸéŸ∞ŸÑŸêŸÉŸé' ‚Üî W[6] (score: 100%)\n",
      "  Anchor 2: Q[2]='Ÿ±ŸÑ€°ŸÉŸêÿ™ŸéŸ∞ÿ®Ÿè' ‚Üî W[7] (score: 80%)\n",
      "  Anchor 3: Q[3]='ŸÑŸéÿß' ‚Üî W[8] (score: 100%)\n",
      "  Anchor 4: Q[4]='ÿ±ŸéŸä€°ÿ®Ÿé€õ' ‚Üî W[9] (score: 100%)\n",
      "\n",
      "üîó Interpolating between anchors...\n",
      "  Segment 0: Q[0‚Üí0] ‚Üî W[4‚Üí47]\n",
      "  Segment 1: Q[0‚Üí1] ‚Üî W[47‚Üí6]\n",
      "  Segment 2: Q[1‚Üí2] ‚Üî W[6‚Üí7]\n",
      "  Segment 3: Q[2‚Üí3] ‚Üî W[7‚Üí8]\n",
      "  Segment 4: Q[3‚Üí4] ‚Üî W[8‚Üí9]\n",
      "  Segment 5: Q[4‚Üí5] ‚Üî W[9‚Üí10]\n",
      "  Segment 6: Q[5‚Üí6] ‚Üî W[10‚Üí11]\n",
      "  Segment 7: Q[6‚Üí7] ‚Üî W[11‚Üí12]\n",
      "  Segment 8: Q[7‚Üí8] ‚Üî W[12‚Üí13]\n",
      "  Segment 9: Q[8‚Üí9] ‚Üî W[13‚Üí14]\n",
      "  Segment 10: Q[9‚Üí10] ‚Üî W[14‚Üí15]\n",
      "  Segment 11: Q[10‚Üí11] ‚Üî W[15‚Üí16]\n",
      "  Segment 12: Q[11‚Üí13] ‚Üî W[16‚Üí18]\n",
      "  Segment 13: Q[13‚Üí14] ‚Üî W[18‚Üí19]\n",
      "  Segment 14: Q[14‚Üí15] ‚Üî W[19‚Üí20]\n",
      "  Segment 15: Q[15‚Üí16] ‚Üî W[20‚Üí21]\n",
      "  Segment 16: Q[16‚Üí17] ‚Üî W[21‚Üí14]\n",
      "  Segment 17: Q[17‚Üí18] ‚Üî W[14‚Üí23]\n",
      "  Segment 18: Q[18‚Üí19] ‚Üî W[23‚Üí24]\n",
      "  Segment 19: Q[19‚Üí20] ‚Üî W[24‚Üí25]\n",
      "  Segment 20: Q[20‚Üí21] ‚Üî W[25‚Üí26]\n",
      "  Segment 21: Q[21‚Üí22] ‚Üî W[26‚Üí24]\n",
      "  Segment 22: Q[22‚Üí23] ‚Üî W[24‚Üí28]\n",
      "  Segment 23: Q[23‚Üí24] ‚Üî W[28‚Üí29]\n",
      "  Segment 24: Q[24‚Üí25] ‚Üî W[29‚Üí30]\n",
      "  Segment 25: Q[25‚Üí26] ‚Üî W[30‚Üí31]\n",
      "  Segment 26: Q[26‚Üí27] ‚Üî W[31‚Üí32]\n",
      "  Segment 27: Q[27‚Üí28] ‚Üî W[32‚Üí33]\n",
      "  Segment 28: Q[28‚Üí29] ‚Üî W[33‚Üí34]\n",
      "  Segment 29: Q[29‚Üí30] ‚Üî W[34‚Üí11]\n",
      "  Segment 30: Q[30‚Üí31] ‚Üî W[11‚Üí28]\n",
      "  Segment 31: Q[31‚Üí32] ‚Üî W[28‚Üí37]\n",
      "  Segment 32: Q[32‚Üí33] ‚Üî W[37‚Üí38]\n",
      "  Segment 33: Q[33‚Üí34] ‚Üî W[38‚Üí31]\n",
      "  Segment 34: Q[34‚Üí35] ‚Üî W[31‚Üí40]\n",
      "  Segment 35: Q[35‚Üí36] ‚Üî W[40‚Üí41]\n",
      "  Segment 36: Q[36‚Üí37] ‚Üî W[41‚Üí13]\n",
      "  Segment 37: Q[37‚Üí38] ‚Üî W[13‚Üí43]\n",
      "  Segment 38: Q[38‚Üí39] ‚Üî W[43‚Üí44]\n",
      "  Segment 39: Q[39‚Üí40] ‚Üî W[44‚Üí45]\n",
      "  Segment 40: Q[40‚Üí41] ‚Üî W[45‚Üí46]\n",
      "  Segment 41: Q[41‚Üí42] ‚Üî W[46‚Üí47]\n",
      "  Segment 42: Q[42‚Üí43] ‚Üî W[47‚Üí48]\n",
      "  Segment 43: Q[43‚Üí44] ‚Üî W[48‚Üí49]\n",
      "  Segment 44: Q[44‚Üí45] ‚Üî W[49‚Üí8]\n",
      "  Segment 45: Q[45‚Üí46] ‚Üî W[8‚Üí14]\n",
      "  Segment 46: Q[46‚Üí47] ‚Üî W[14‚Üí52]\n",
      "  Segment 47: Q[47‚Üí48] ‚Üî W[52‚Üí53]\n",
      "  Segment 48: Q[48‚Üí49] ‚Üî W[53‚Üí34]\n",
      "  Segment 49: Q[49‚Üí50] ‚Üî W[34‚Üí55]\n",
      "  Segment 50: Q[50‚Üí51] ‚Üî W[55‚Üí56]\n",
      "  Segment 51: Q[51‚Üí52] ‚Üî W[56‚Üí57]\n",
      "  Segment 52: Q[52‚Üí53] ‚Üî W[57‚Üí56]\n",
      "  Segment 53: Q[53‚Üí54] ‚Üî W[56‚Üí59]\n",
      "  Segment 54: Q[54‚Üí55] ‚Üî W[59‚Üí60]\n",
      "  Segment 55: Q[55‚Üí56] ‚Üî W[60‚Üí61]\n",
      "  Segment 56: Q[56‚Üí57] ‚Üî W[61‚Üí62]\n",
      "  Segment 57: Q[57‚Üí58] ‚Üî W[62‚Üí63]\n",
      "  Segment 58: Q[58‚Üí59] ‚Üî W[63‚Üí64]\n",
      "  Segment 59: Q[59‚Üí60] ‚Üî W[64‚Üí65]\n",
      "  Segment 60: Q[60‚Üí61] ‚Üî W[65‚Üí28]\n",
      "  Segment 61: Q[61‚Üí62] ‚Üî W[28‚Üí67]\n",
      "  Segment 62: Q[62‚Üí63] ‚Üî W[67‚Üí68]\n",
      "  Segment 63: Q[63‚Üí64] ‚Üî W[68‚Üí69]\n",
      "  Segment 64: Q[64‚Üí65] ‚Üî W[69‚Üí70]\n",
      "  Segment 65: Q[65‚Üí66] ‚Üî W[70‚Üí71]\n",
      "  Segment 66: Q[66‚Üí67] ‚Üî W[71‚Üí26]\n",
      "  Segment 67: Q[67‚Üí68] ‚Üî W[26‚Üí31]\n",
      "  Segment 68: Q[68‚Üí69] ‚Üî W[31‚Üí74]\n",
      "  Segment 69: Q[69‚Üí70] ‚Üî W[74‚Üí80]\n",
      "  Segment 70: Q[70‚Üí71] ‚Üî W[80‚Üí53]\n",
      "  Segment 71: Q[71‚Üí72] ‚Üî W[53‚Üí21]\n",
      "  Segment 72: Q[72‚Üí73] ‚Üî W[21‚Üí78]\n",
      "  Segment 73: Q[73‚Üí74] ‚Üî W[78‚Üí26]\n",
      "  Segment 74: Q[74‚Üí75] ‚Üî W[26‚Üí80]\n",
      "  Segment 75: Q[75‚Üí76] ‚Üî W[80‚Üí81]\n",
      "  Segment 76: Q[76‚Üí77] ‚Üî W[81‚Üí82]\n",
      "  Segment 77: Q[77‚Üí78] ‚Üî W[82‚Üí26]\n",
      "  Segment 78: Q[78‚Üí79] ‚Üî W[26‚Üí84]\n",
      "  Segment 79: Q[79‚Üí80] ‚Üî W[84‚Üí85]\n",
      "  Segment 80: Q[80‚Üí81] ‚Üî W[85‚Üí55]\n",
      "  Segment 81: Q[81‚Üí82] ‚Üî W[55‚Üí87]\n",
      "  Segment 82: Q[82‚Üí83] ‚Üî W[87‚Üí88]\n",
      "  Segment 83: Q[83‚Üí84] ‚Üî W[88‚Üí53]\n",
      "  Segment 84: Q[84‚Üí85] ‚Üî W[53‚Üí90]\n",
      "  Segment 85: Q[85‚Üí86] ‚Üî W[90‚Üí61]\n",
      "  Segment 86: Q[86‚Üí87] ‚Üî W[61‚Üí62]\n",
      "  Segment 87: Q[87‚Üí88] ‚Üî W[62‚Üí93]\n",
      "  Segment 88: Q[88‚Üí89] ‚Üî W[93‚Üí23]\n",
      "  Segment 89: Q[89‚Üí90] ‚Üî W[23‚Üí95]\n",
      "  Segment 90: Q[90‚Üí91] ‚Üî W[95‚Üí96]\n",
      "  Segment 91: Q[91‚Üí92] ‚Üî W[96‚Üí97]\n",
      "  Segment 92: Q[92‚Üí93] ‚Üî W[97‚Üí98]\n",
      "  Segment 93: Q[93‚Üí94] ‚Üî W[98‚Üí99]\n",
      "  Segment 94: Q[94‚Üí95] ‚Üî W[99‚Üí8]\n",
      "  Segment 95: Q[95‚Üí96] ‚Üî W[8‚Üí101]\n",
      "  Segment 96: Q[96‚Üí97] ‚Üî W[101‚Üí85]\n",
      "  Segment 97: Q[97‚Üí98] ‚Üî W[85‚Üí103]\n",
      "  Segment 98: Q[98‚Üí99] ‚Üî W[103‚Üí104]\n",
      "  Segment 99: Q[99‚Üí100] ‚Üî W[104‚Üí105]\n",
      "  Segment 100: Q[100‚Üí101] ‚Üî W[105‚Üí106]\n",
      "  Segment 101: Q[101‚Üí102] ‚Üî W[106‚Üí107]\n",
      "  Segment 102: Q[102‚Üí103] ‚Üî W[107‚Üí81]\n",
      "  Segment 103: Q[103‚Üí104] ‚Üî W[81‚Üí109]\n",
      "  Segment 104: Q[104‚Üí105] ‚Üî W[109‚Üí31]\n",
      "  Segment 105: Q[105‚Üí106] ‚Üî W[31‚Üí111]\n",
      "  Segment 106: Q[106‚Üí107] ‚Üî W[111‚Üí112]\n",
      "  Segment 107: Q[107‚Üí108] ‚Üî W[112‚Üí8]\n",
      "  Segment 108: Q[108‚Üí109] ‚Üî W[8‚Üí84]\n",
      "  Segment 109: Q[109‚Üí110] ‚Üî W[84‚Üí97]\n",
      "  Segment 110: Q[110‚Üí111] ‚Üî W[97‚Üí98]\n",
      "  Segment 111: Q[111‚Üí112] ‚Üî W[98‚Üí99]\n",
      "  Segment 112: Q[112‚Üí113] ‚Üî W[99‚Üí78]\n",
      "  Segment 113: Q[113‚Üí114] ‚Üî W[78‚Üí119]\n",
      "  Segment 114: Q[114‚Üí115] ‚Üî W[119‚Üí120]\n",
      "  Segment 115: Q[115‚Üí116] ‚Üî W[120‚Üí65]\n",
      "  Segment 116: Q[116‚Üí117] ‚Üî W[65‚Üí104]\n",
      "  Segment 117: Q[117‚Üí118] ‚Üî W[104‚Üí123]\n",
      "  Segment 118: Q[118‚Üí119] ‚Üî W[123‚Üí119]\n",
      "  Segment 119: Q[119‚Üí120] ‚Üî W[119‚Üí120]\n",
      "  Segment 120: Q[120‚Üí121] ‚Üî W[120‚Üí126]\n",
      "  Segment 121: Q[121‚Üí122] ‚Üî W[126‚Üí81]\n",
      "  Segment 122: Q[122‚Üí123] ‚Üî W[81‚Üí109]\n",
      "  Segment 123: Q[123‚Üí124] ‚Üî W[109‚Üí31]\n",
      "  Segment 124: Q[124‚Üí125] ‚Üî W[31‚Üí126]\n",
      "  Segment 125: Q[125‚Üí126] ‚Üî W[126‚Üí112]\n",
      "  Segment 126: Q[126‚Üí127] ‚Üî W[112‚Üí8]\n",
      "  Segment 127: Q[127‚Üí128] ‚Üî W[8‚Üí133]\n",
      "  Segment 128: Q[128‚Üí129] ‚Üî W[133‚Üí97]\n",
      "  Segment 129: Q[129‚Üí130] ‚Üî W[97‚Üí135]\n",
      "  Segment 130: Q[130‚Üí131] ‚Üî W[135‚Üí13]\n",
      "  Segment 131: Q[131‚Üí132] ‚Üî W[13‚Üí78]\n",
      "  Segment 132: Q[132‚Üí133] ‚Üî W[78‚Üí104]\n",
      "  Segment 133: Q[133‚Üí134] ‚Üî W[104‚Üí68]\n",
      "  Segment 134: Q[134‚Üí135] ‚Üî W[68‚Üí97]\n",
      "  Segment 135: Q[135‚Üí136] ‚Üî W[97‚Üí141]\n",
      "  Segment 136: Q[136‚Üí137] ‚Üî W[141‚Üí142]\n",
      "  Segment 137: Q[137‚Üí138] ‚Üî W[142‚Üí143]\n",
      "  Segment 138: Q[138‚Üí139] ‚Üî W[143‚Üí104]\n",
      "  Segment 139: Q[139‚Üí140] ‚Üî W[104‚Üí145]\n",
      "  Segment 140: Q[140‚Üí141] ‚Üî W[145‚Üí146]\n",
      "  Segment 141: Q[141‚Üí142] ‚Üî W[146‚Üí105]\n",
      "  Segment 142: Q[142‚Üí143] ‚Üî W[105‚Üí106]\n",
      "  Segment 143: Q[143‚Üí144] ‚Üî W[106‚Üí149]\n",
      "  Segment 144: Q[144‚Üí145] ‚Üî W[149‚Üí53]\n",
      "  Segment 145: Q[145‚Üí147] ‚Üî W[53‚Üí37]\n",
      "  Segment 146: Q[147‚Üí149] ‚Üî W[37‚Üí85]\n",
      "  Segment 147: Q[149‚Üí151] ‚Üî W[85‚Üí133]\n",
      "  Segment 148: Q[151‚Üí152] ‚Üî W[133‚Üí33]\n",
      "  Segment 149: Q[152‚Üí153] ‚Üî W[33‚Üí13]\n",
      "  Segment 150: Q[153‚Üí160] ‚Üî W[13‚Üí26]\n",
      "  Segment 151: Q[160‚Üí161] ‚Üî W[26‚Üí95]\n",
      "  Segment 152: Q[161‚Üí170] ‚Üî W[95‚Üí23]\n",
      "  Segment 153: Q[170‚Üí173] ‚Üî W[23‚Üí53]\n",
      "  Segment 154: Q[173‚Üí176] ‚Üî W[53‚Üí85]\n",
      "  Segment 155: Q[176‚Üí178] ‚Üî W[85‚Üí8]\n",
      "  Segment 156: Q[178‚Üí183] ‚Üî W[8‚Üí31]\n",
      "  Segment 157: Q[183‚Üí184] ‚Üî W[31‚Üí8]\n",
      "  Segment 158: Q[184‚Üí188] ‚Üî W[8‚Üí28]\n",
      "  Segment 159: Q[188‚Üí190] ‚Üî W[28‚Üí10]\n",
      "  Segment 160: Q[190‚Üí194] ‚Üî W[10‚Üí133]\n",
      "  Segment 161: Q[194‚Üí196] ‚Üî W[133‚Üí85]\n",
      "  Segment 162: Q[196‚Üí198] ‚Üî W[85‚Üí28]\n",
      "  Segment 163: Q[198‚Üí200] ‚Üî W[28‚Üí150]\n",
      "\n",
      "‚úÖ Aligned 200 words\n",
      "\n",
      "üìä Sample alignments:\n",
      "  [0] 'ÿßŸÑŸìŸÖŸì' 72180ms to 72580ms (400ms)\n",
      "  [1] 'ÿ∞ŸéŸ∞ŸÑŸêŸÉŸé' 13580ms to 15540ms (1960ms)\n",
      "  [2] 'Ÿ±ŸÑ€°ŸÉŸêÿ™ŸéŸ∞ÿ®Ÿè' 14540ms to 15960ms (1420ms)\n",
      "  [50] 'ŸÇŸèŸÑŸèŸàÿ®ŸêŸáŸêŸÖ€°' 79760ms to 81760ms (2000ms)\n",
      "  [100] 'ÿ•ŸêŸÜŸëŸéŸÖŸéÿß' 139920ms to 142920ms (3000ms)\n",
      "  [150] 'ÿ∑Ÿèÿ∫€°ŸäŸéŸ∞ŸÜŸêŸáŸêŸÖ€°' 147820ms to 182300ms (34480ms)\n",
      "  [198] 'ŸÖŸëŸêŸÜŸé' 45200ms to 125120ms (79920ms)\n",
      "  [199] 'Ÿ±ŸÑÿµŸëŸéŸàŸéŸ∞ÿπŸêŸÇŸê' 124380ms to 201480ms (77100ms)\n"
     ]
    }
   ],
   "source": [
    "# BLOCK 3: ANCHOR-BASED ALIGNMENT\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 2: ANCHOR-BASED ALIGNMENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def normalize_arabic(text):\n",
    "    \"\"\"Strip everything except base letters\"\"\"\n",
    "    text = re.sub(r'[^\\u0621-\\u063A\\u0641-\\u064A]', '', text)\n",
    "    text = text.replace('ÿ£', 'ÿß').replace('ÿ•', 'ÿß').replace('ÿ¢', 'ÿß').replace('Ÿ±', 'ÿß')\n",
    "    text = text.replace('ÿ©', 'Ÿá').replace('Ÿâ', 'Ÿä').replace('ÿ§', 'Ÿà').replace('ÿ¶', 'Ÿä')\n",
    "    return text\n",
    "\n",
    "with open(text_file, 'r', encoding='utf-8') as f:\n",
    "    quran_text = f.read()\n",
    "\n",
    "quran_text = re.sub(r'\\s*\\(\\d+\\)\\s*', ' ', quran_text)\n",
    "all_quran_words = quran_text.split()\n",
    "word_limit = len(whisper_words) + 50\n",
    "quran_words = all_quran_words[:word_limit]\n",
    "\n",
    "whisper_normalized = [normalize_arabic(w['word']) for w in whisper_words]\n",
    "quran_normalized = [normalize_arabic(w) for w in quran_words]\n",
    "\n",
    "# Skip Basmala\n",
    "whisper_start = 0\n",
    "basmala_norm = [normalize_arabic(w) for w in ['ÿ®ÿ≥ŸÖ', 'ÿßŸÑŸÑŸá', 'ÿßŸÑÿ±ÿ≠ŸÖŸÜ', 'ÿßŸÑÿ±ÿ≠ŸäŸÖ']]\n",
    "if len(whisper_words) >= 4:\n",
    "    if [whisper_normalized[i] for i in range(4)] == basmala_norm:\n",
    "        if normalize_arabic(quran_words[0]) != basmala_norm[0]:\n",
    "            whisper_start = 4\n",
    "            print(\"‚úì Skipped Basmala\")\n",
    "\n",
    "print(f\"\\nQuran: {len(quran_words)} words | Whisper: {len(whisper_words)} words\")\n",
    "\n",
    "# STEP 1: Find ALL possible matches with scores\n",
    "print(\"\\nüîç Finding all possible matches...\")\n",
    "matches = []  # (q_idx, w_idx, num_combined, score)\n",
    "\n",
    "for q_idx in range(len(quran_words)):\n",
    "    q_norm = quran_normalized[q_idx]\n",
    "    best_for_this_q = None\n",
    "    \n",
    "    for w_idx in range(whisper_start, len(whisper_words)):\n",
    "        for num_combine in range(1, min(7, len(whisper_words) - w_idx + 1)):\n",
    "            combined = \"\".join(whisper_normalized[w_idx:w_idx + num_combine])\n",
    "            score = fuzz.ratio(q_norm, combined)\n",
    "            \n",
    "            if not best_for_this_q or score > best_for_this_q[3]:\n",
    "                best_for_this_q = (q_idx, w_idx, num_combine, score)\n",
    "    \n",
    "    if best_for_this_q:\n",
    "        matches.append(best_for_this_q)\n",
    "\n",
    "# STEP 2: Find ANCHORS (high confidence matches)\n",
    "print(\"\\n‚öì Finding anchor points...\")\n",
    "anchors = []\n",
    "for q_idx, w_idx, num_combine, score in matches:\n",
    "    if score >= 80:  # High confidence\n",
    "        anchors.append((q_idx, w_idx, num_combine, score))\n",
    "\n",
    "anchors.sort(key=lambda x: x[0])  # Sort by Quran index\n",
    "print(f\"Found {len(anchors)} anchor points (‚â•80% confidence)\")\n",
    "\n",
    "# Show anchors\n",
    "for i in range(min(5, len(anchors))):\n",
    "    q_idx, w_idx, num_combine, score = anchors[i]\n",
    "    print(f\"  Anchor {i}: Q[{q_idx}]='{quran_words[q_idx]}' ‚Üî W[{w_idx}] (score: {score:.0f}%)\")\n",
    "\n",
    "# STEP 3: Interpolate between anchors\n",
    "print(\"\\nüîó Interpolating between anchors...\")\n",
    "aligned = []\n",
    "\n",
    "if not anchors:\n",
    "    print(\"‚ùå No anchors found! Using best guesses...\")\n",
    "    # Fallback: use all matches\n",
    "    for q_idx, w_idx, num_combine, score in matches:\n",
    "        if w_idx + num_combine - 1 < len(whisper_words):\n",
    "            aligned.append({\n",
    "                \"word\": quran_words[q_idx],\n",
    "                \"start_ms\": whisper_words[w_idx][\"start_ms\"],\n",
    "                \"end_ms\": whisper_words[w_idx + num_combine - 1][\"end_ms\"]\n",
    "            })\n",
    "else:\n",
    "    # Add start anchor (beginning)\n",
    "    anchors.insert(0, (0, whisper_start, 1, 100))\n",
    "    # Add end anchor\n",
    "    anchors.append((len(quran_words), len(whisper_words), 1, 100))\n",
    "    \n",
    "    # Process each segment between anchors\n",
    "    for i in range(len(anchors) - 1):\n",
    "        anchor1_q, anchor1_w, _, _ = anchors[i]\n",
    "        anchor2_q, anchor2_w, _, _ = anchors[i + 1]\n",
    "        \n",
    "        quran_gap = anchor2_q - anchor1_q\n",
    "        whisper_gap = anchor2_w - anchor1_w\n",
    "        \n",
    "        print(f\"  Segment {i}: Q[{anchor1_q}‚Üí{anchor2_q}] ‚Üî W[{anchor1_w}‚Üí{anchor2_w}]\")\n",
    "        \n",
    "        if quran_gap == 0:\n",
    "            continue\n",
    "        \n",
    "        # Proportionally distribute Whisper words\n",
    "        for j in range(quran_gap):\n",
    "            q_idx = anchor1_q + j\n",
    "            if q_idx >= len(quran_words):\n",
    "                break\n",
    "            \n",
    "            # Calculate proportional Whisper position\n",
    "            ratio = j / quran_gap if quran_gap > 0 else 0\n",
    "            w_start_idx = anchor1_w + int(ratio * whisper_gap)\n",
    "            w_end_idx = anchor1_w + int((j + 1) / quran_gap * whisper_gap)\n",
    "            \n",
    "            # Ensure valid indices\n",
    "            w_start_idx = max(whisper_start, min(w_start_idx, len(whisper_words) - 1))\n",
    "            w_end_idx = max(w_start_idx, min(w_end_idx, len(whisper_words) - 1))\n",
    "            \n",
    "            if w_start_idx < len(whisper_words) and w_end_idx < len(whisper_words):\n",
    "                aligned.append({\n",
    "                    \"word\": quran_words[q_idx],\n",
    "                    \"start_ms\": whisper_words[w_start_idx][\"start_ms\"],\n",
    "                    \"end_ms\": whisper_words[w_end_idx][\"end_ms\"]\n",
    "                })\n",
    "\n",
    "print(f\"\\n‚úÖ Aligned {len(aligned)} words\")\n",
    "\n",
    "# Show samples\n",
    "print(\"\\nüìä Sample alignments:\")\n",
    "for i in [0, 1, 2, len(aligned)//4, len(aligned)//2, len(aligned)*3//4, len(aligned)-2, len(aligned)-1]:\n",
    "    if 0 <= i < len(aligned):\n",
    "        w = aligned[i]\n",
    "        dur = w['end_ms'] - w['start_ms']\n",
    "        print(f\"  [{i}] '{w['word']}' {w['start_ms']}ms to {w['end_ms']}ms ({dur}ms)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 3: SAVING\n",
      "============================================================\n",
      "\n",
      "Saved: bakara_timings.json\n",
      "\n",
      "DONE! Use in HTML:\n",
      "  1. Upload audio: bakara.mp3\n",
      "  2. Upload JSON: bakara_timings.json\n",
      "  3. Upload text: baqara.txt\n"
     ]
    }
   ],
   "source": [
    "# BLOCK 4: SAVE OUTPUT\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 3: SAVING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "with open(output_json, 'w', encoding='utf-8') as f:\n",
    "    json.dump(aligned, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\nSaved: {output_json}\")\n",
    "print(f\"\\nDONE! Use in HTML:\")\n",
    "print(f\"  1. Upload audio: {audio_file}\")\n",
    "print(f\"  2. Upload JSON: {output_json}\")\n",
    "print(f\"  3. Upload text: {text_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
